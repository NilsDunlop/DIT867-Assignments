{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_finalTrain: (40054,)\n",
      "Size of Y_finalTrain: (40054,)\n",
      "Size of X_finalTest: (10014,)\n",
      "Size of Y_finalTest: (10014,)\n",
      "Number of annotators per asset in Y_Train:\n",
      "0\n",
      "1/1                            19248\n",
      "0/0                            18221\n",
      "0/-1                            2434\n",
      "1/-1                            2082\n",
      "1/0                             1467\n",
      "                               ...  \n",
      "0/1/0/0/0                          1\n",
      "0/0/1/-1                           1\n",
      "1/1/1/1/1/1/1/1/1/0/1/1/1/1        1\n",
      "0/1/0/1/1/0/0/1                    1\n",
      "1/0/-1/-1                          1\n",
      "Name: count, Length: 190, dtype: int64\n",
      "Length of string per row in finalTrain[0]:\n",
      "0        3\n",
      "1        4\n",
      "2        3\n",
      "3        4\n",
      "4        3\n",
      "        ..\n",
      "50063    3\n",
      "50064    3\n",
      "50065    3\n",
      "50066    3\n",
      "50067    3\n",
      "Name: 0, Length: 50068, dtype: int64\n",
      "Row with the maximum number of annotators (index): 35297\n",
      "Count of annotators in the maximum row: 113\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "firstSample = pd.read_csv(\"a3_first_sample.txt\", sep='\\t', header=None)\n",
    "finalTrain = pd.read_csv(\"a3_train_final.txt\", sep='\\t', header=None)\n",
    "finalTest = pd.read_csv(\"a3_test_final.txt\", sep='\\t', header=None)\n",
    "\n",
    "#Shuffle the data\n",
    "#finalTrain = finalTrain.sample(frac=1.0, random_state=0)\n",
    "#finalTest = finalTrain.sample(frac=1.0, random_state=0)\n",
    "\n",
    "##Features \n",
    "X_firstSample = firstSample[1]\n",
    "X_Train = finalTrain[1]\n",
    "X_Test = finalTest[1]\n",
    "\n",
    "#Labels\n",
    "Y_firstSample = firstSample[0]\n",
    "Y_Train = finalTrain[0]\n",
    "Y_Test = finalTest[0]\n",
    "\n",
    "# Split into input part X and output part Y.\n",
    "X_finalTrain, X_finalTest, Y_finalTrain, Y_finalTest= train_test_split(X_Train, Y_Train, test_size=0.2, random_state=0)\n",
    "print(\"Size of X_finalTrain:\", X_finalTrain.shape)\n",
    "print(\"Size of Y_finalTrain:\", Y_finalTrain.shape)\n",
    "print(\"Size of X_finalTest:\", X_finalTest.shape)\n",
    "print(\"Size of Y_finalTest:\", Y_finalTest.shape)\n",
    "\n",
    "num_annotators = Y_Train.value_counts()\n",
    "print(\"Number of annotators per asset in Y_Train:\")\n",
    "print(num_annotators)\n",
    "lengths = finalTrain[0].apply(len)\n",
    "print(\"Length of string per row in finalTrain[0]:\")\n",
    "print(lengths)\n",
    "\n",
    "max_length_index = lengths.idxmax()\n",
    "print(\"Row with the maximum number of annotators (index):\", max_length_index)\n",
    "max_length = lengths[max_length_index]\n",
    "print(\"Count of annotators in the maximum row:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56041541841422\n"
     ]
    }
   ],
   "source": [
    "#Assessing using Logistic Regression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "vecTfidf = TfidfVectorizer(max_features=2000, max_df = 0.9, min_df=0.01, lowercase = True )\n",
    "X_finalTrain_tfidf = vecTfidf.fit_transform(X_finalTrain)\n",
    "X_Test_tfidf = vecTfidf.transform(X_finalTest)\n",
    "\n",
    "#Types of Text Classifiers Models \n",
    "LRModel = LogisticRegression(max_iter=300)\n",
    "LRModel.fit(X_finalTrain_tfidf, Y_finalTrain)\n",
    "Y_TestTfidfLRModel = LRModel.predict(X_Test_tfidf)\n",
    "accuracy_LRModel = accuracy_score(Y_finalTest, Y_TestTfidfLRModel) \n",
    "print(\"Accuracy:\", accuracy_LRModel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 35.2 GiB for an array with shape (4718597680,) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#Types of Text Classifiers Models \u001b[39;00m\n\u001b[0;32m     11\u001b[0m LRHashModel \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mLRHashModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_finalTrain_Hash\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_finalTrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m Y_TestHashLRModel \u001b[38;5;241m=\u001b[39m LRHashModel\u001b[38;5;241m.\u001b[39mpredict(X_Test_Hash)\n\u001b[0;32m     14\u001b[0m accuracy_LRHash \u001b[38;5;241m=\u001b[39m accuracy_score(Y_finalTest, Y_TestHashLRModel) \n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1303\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1301\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1303\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1328\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:452\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    448\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C\n\u001b[0;32m    449\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[0;32m    450\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[0;32m    451\u001b[0m ]\n\u001b[1;32m--> 452\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    460\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[0;32m    461\u001b[0m     solver,\n\u001b[0;32m    462\u001b[0m     opt_res,\n\u001b[0;32m    463\u001b[0m     max_iter,\n\u001b[0;32m    464\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[0;32m    465\u001b[0m )\n\u001b[0;32m    466\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:338\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    336\u001b[0m f \u001b[38;5;241m=\u001b[39m array(\u001b[38;5;241m0.0\u001b[39m, float64)\n\u001b[0;32m    337\u001b[0m g \u001b[38;5;241m=\u001b[39m zeros((n,), float64)\n\u001b[1;32m--> 338\u001b[0m wa \u001b[38;5;241m=\u001b[39m zeros(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mm\u001b[38;5;241m*\u001b[39mn \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m5\u001b[39m\u001b[38;5;241m*\u001b[39mn \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m11\u001b[39m\u001b[38;5;241m*\u001b[39mm\u001b[38;5;241m*\u001b[39mm \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m8\u001b[39m\u001b[38;5;241m*\u001b[39mm, float64)\n\u001b[0;32m    339\u001b[0m iwa \u001b[38;5;241m=\u001b[39m zeros(\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mn, fortran_int)\n\u001b[0;32m    340\u001b[0m task \u001b[38;5;241m=\u001b[39m zeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS60\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 35.2 GiB for an array with shape (4718597680,) and data type float64"
     ]
    }
   ],
   "source": [
    "#Assessing using Logistic Regression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "vecHash = HashingVectorizer()\n",
    "X_finalTrain_Hash = vecHash.fit_transform(X_finalTrain)\n",
    "X_Test_Hash = vecHash.transform(X_finalTest)\n",
    "\n",
    "#Types of Text Classifiers Models \n",
    "LRHashModel = LogisticRegression(max_iter=300)\n",
    "LRHashModel.fit(X_finalTrain_Hash, Y_finalTrain)\n",
    "Y_TestHashLRModel = LRHashModel.predict(X_Test_Hash)\n",
    "accuracy_LRHash = accuracy_score(Y_finalTest, Y_TestHashLRModel) \n",
    "print(\"Accuracy:\", accuracy_LRHash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Encode sentences for training and testing\u001b[39;00m\n\u001b[0;32m     14\u001b[0m X_train_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(X_finalTrain)\n\u001b[1;32m---> 15\u001b[0m X_test_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_finalTest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Train a classifier (Logistic Regression)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m LRBert \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:284\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    281\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 284\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    287\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:98\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m     96\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 98\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    101\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:464\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 464\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    466\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Reset index to ensure it's contiguous\n",
    "X_finalTrain.reset_index(drop=True, inplace=True)\n",
    "X_finalTest.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Encode sentences for training and testing\n",
    "X_train_embeddings = model.encode(X_finalTrain)\n",
    "X_test_embeddings = model.encode(X_finalTest)\n",
    "\n",
    "# Train a classifier (Logistic Regression)\n",
    "LRBert = LogisticRegression(max_iter=1000)\n",
    "LRBert.fit(X_train_embeddings, Y_finalTrain)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred = LRBert.predict(X_test_embeddings)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_BERT = accuracy_score(Y_finalTest, Y_pred)\n",
    "print(\"Accuracy:\", accuracy_BERT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [40054, 50068]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#Fiting the Dummy Classifier to the TdIdf Vectoriser\u001b[39;00m\n\u001b[0;32m     14\u001b[0m dummy_clf \u001b[38;5;241m=\u001b[39m DummyClassifier(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmost_frequent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mdummy_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_finalTrain_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_finalTrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m Y_tfidfCLF\u001b[38;5;241m=\u001b[39mdummy_clf\u001b[38;5;241m.\u001b[39mpredict(X_finalTest)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_score(Y_finalTest, Y_tfidfCLF))\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\dummy.py:198\u001b[0m, in \u001b[0;36mDummyClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    194\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(y, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 198\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32mc:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [40054, 50068]"
     ]
    }
   ],
   "source": [
    "#Trivial Baseline Comparison of Chosen Vectoriser --> Dummy Classifer to TdIdf Vectoriser\n",
    "vecTfidf = TfidfVectorizer()\n",
    "#Vectorising the Training Data\n",
    "vecCount = CountVectorizer()\n",
    "vecTfidf = TfidfVectorizer()\n",
    "vecHash = HashingVectorizer()\n",
    "\n",
    "Y_finalTrain = finalTrain[0]\n",
    "X_finalTrain_count= vecCount.fit_transform(X_finalTrain)\n",
    "X_finalTrain_tfidf = vecTfidf.fit_transform(X_finalTrain)\n",
    "X_finalTrain_hash = vecHash.fit_transform(X_finalTrain)\n",
    "\n",
    "#Fiting the Dummy Classifier to the TdIdf Vectoriser\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_finalTrain_tfidf, Y_finalTrain)\n",
    "Y_tfidfCLF=dummy_clf.predict(X_finalTest)\n",
    "print(accuracy_score(Y_finalTest, Y_tfidfCLF))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.67      1.00      0.80         2\n",
      "     class 1       0.00      0.00      0.00         1\n",
      "     class 2       0.67      0.67      0.67         3\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.44      0.56      0.49         6\n",
      "weighted avg       0.56      0.67      0.60         6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\3742\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### CHECKING THE ACCURACY OF THE MODEL USING THE CONFUSION MATRIX\n",
    "#Initiating the confusion matrix\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "confusion_matrix(y_true, y_pred)\n",
    "\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "### Displaying the confusion matrix\n",
    "X, y = make_classification(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "clf = SVC(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "SVC(random_state=0)\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
