{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e02c6b96b85df5a1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Assignment 4\n",
    "## Group Members:\n",
    "### Nils Dunlop, e-mail: gusdunlni@student.gu.se\n",
    "### Francisco Alejandro Erazo Piza, e-mail: guserafr@student.gu.se\n",
    "### Chukwudumebi Ubogu, e-mail: gusuboch@student.gu.se\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d86d47ab6dab10b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 1: A small linear regression example in PyTorch\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c27312703ea6f271",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T16:40:56.490098100Z",
     "start_time": "2024-02-25T16:40:54.359230200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.76405235,  0.40015721],\n",
       "        [ 0.97873798,  2.2408932 ],\n",
       "        [ 1.86755799, -0.97727788],\n",
       "        [ 0.95008842, -0.15135721],\n",
       "        [-0.10321885,  0.4105985 ],\n",
       "        [ 0.14404357,  1.45427351],\n",
       "        [ 0.76103773,  0.12167502],\n",
       "        [ 0.44386323,  0.33367433],\n",
       "        [ 1.49407907, -0.20515826],\n",
       "        [ 0.3130677 , -0.85409574],\n",
       "        [-2.55298982,  0.6536186 ],\n",
       "        [ 0.8644362 , -0.74216502],\n",
       "        [ 2.26975462, -1.45436567],\n",
       "        [ 0.04575852, -0.18718385],\n",
       "        [ 1.53277921,  1.46935877],\n",
       "        [ 0.15494743,  0.37816252],\n",
       "        [-0.88778575, -1.98079647],\n",
       "        [-0.34791215,  0.15634897],\n",
       "        [ 1.23029068,  1.20237985],\n",
       "        [-0.38732682, -0.30230275],\n",
       "        [-1.04855297, -1.42001794],\n",
       "        [-1.70627019,  1.9507754 ],\n",
       "        [-0.50965218, -0.4380743 ],\n",
       "        [-1.25279536,  0.77749036],\n",
       "        [-1.61389785, -0.21274028],\n",
       "        [-0.89546656,  0.3869025 ],\n",
       "        [-0.51080514, -1.18063218],\n",
       "        [-0.02818223,  0.42833187],\n",
       "        [ 0.06651722,  0.3024719 ],\n",
       "        [-0.63432209, -0.36274117],\n",
       "        [-0.67246045, -0.35955316],\n",
       "        [-0.81314628, -1.7262826 ],\n",
       "        [ 0.17742614, -0.40178094],\n",
       "        [-1.63019835,  0.46278226],\n",
       "        [-0.90729836,  0.0519454 ],\n",
       "        [ 0.72909056,  0.12898291],\n",
       "        [ 1.13940068, -1.23482582],\n",
       "        [ 0.40234164, -0.68481009],\n",
       "        [-0.87079715, -0.57884966],\n",
       "        [-0.31155253,  0.05616534],\n",
       "        [-1.16514984,  0.90082649],\n",
       "        [ 0.46566244, -1.53624369],\n",
       "        [ 1.48825219,  1.89588918],\n",
       "        [ 1.17877957, -0.17992484],\n",
       "        [-1.07075262,  1.05445173],\n",
       "        [-0.40317695,  1.22244507],\n",
       "        [ 0.20827498,  0.97663904],\n",
       "        [ 0.3563664 ,  0.70657317],\n",
       "        [ 0.01050002,  1.78587049],\n",
       "        [ 0.12691209,  0.40198936],\n",
       "        [ 1.8831507 , -1.34775906],\n",
       "        [-1.270485  ,  0.96939671],\n",
       "        [-1.17312341,  1.94362119],\n",
       "        [-0.41361898, -0.74745481],\n",
       "        [ 1.92294203,  1.48051479],\n",
       "        [ 1.86755896,  0.90604466],\n",
       "        [-0.86122569,  1.91006495],\n",
       "        [-0.26800337,  0.8024564 ],\n",
       "        [ 0.94725197, -0.15501009],\n",
       "        [ 0.61407937,  0.92220667],\n",
       "        [ 0.37642553, -1.09940079],\n",
       "        [ 0.29823817,  1.3263859 ],\n",
       "        [-0.69456786, -0.14963454],\n",
       "        [-0.43515355,  1.84926373],\n",
       "        [ 0.67229476,  0.40746184],\n",
       "        [-0.76991607,  0.53924919],\n",
       "        [-0.67433266,  0.03183056],\n",
       "        [-0.63584608,  0.67643329],\n",
       "        [ 0.57659082, -0.20829876],\n",
       "        [ 0.39600671, -1.09306151],\n",
       "        [-1.49125759,  0.4393917 ],\n",
       "        [ 0.1666735 ,  0.63503144],\n",
       "        [ 2.38314477,  0.94447949],\n",
       "        [-0.91282223,  1.11701629],\n",
       "        [-1.31590741, -0.4615846 ],\n",
       "        [-0.06824161,  1.71334272],\n",
       "        [-0.74475482, -0.82643854],\n",
       "        [-0.09845252, -0.66347829],\n",
       "        [ 1.12663592, -1.07993151],\n",
       "        [-1.14746865, -0.43782004],\n",
       "        [-0.49803245,  1.92953205],\n",
       "        [ 0.94942081,  0.08755124],\n",
       "        [-1.22543552,  0.84436298],\n",
       "        [-1.00021535, -1.5447711 ],\n",
       "        [ 1.18802979,  0.31694261],\n",
       "        [ 0.92085882,  0.31872765],\n",
       "        [ 0.85683061, -0.65102559],\n",
       "        [-1.03424284,  0.68159452],\n",
       "        [-0.80340966, -0.68954978],\n",
       "        [-0.4555325 ,  0.01747916],\n",
       "        [-0.35399391, -1.37495129],\n",
       "        [-0.6436184 , -2.22340315],\n",
       "        [ 0.62523145, -1.60205766],\n",
       "        [-1.10438334,  0.05216508],\n",
       "        [-0.739563  ,  1.5430146 ],\n",
       "        [-1.29285691,  0.26705087],\n",
       "        [-0.03928282, -1.1680935 ],\n",
       "        [ 0.52327666, -0.17154633],\n",
       "        [ 0.77179055,  0.82350415],\n",
       "        [ 2.16323595,  1.33652795]]),\n",
       " array([ 0.53895342,  0.21594929,  0.85495138,  0.5636907 ,  0.32130011,\n",
       "         0.02282172,  0.43355955,  0.28494948,  0.62455331,  0.457357  ,\n",
       "        -0.09488842,  0.58031413,  0.94447131,  0.30172726,  0.32449838,\n",
       "         0.26373822,  0.38526199,  0.28157172,  0.62281853,  0.28318203,\n",
       "         0.20235347, -0.2192494 ,  0.2351877 ,  0.069649  , -0.0850409 ,\n",
       "         0.13153761,  0.37868332,  0.29131062,  0.23830441,  0.2282746 ,\n",
       "         0.10282648,  0.32232423,  0.3400722 ,  0.03356524,  0.04442704,\n",
       "         0.50787709,  0.79897741,  0.25672869,  0.27826382,  0.32854869,\n",
       "        -0.04091009,  0.52842224,  0.35116701,  0.51040517, -0.00891893,\n",
       "        -0.05060942,  0.36320961,  0.4108211 ,  0.04400282,  0.15071379,\n",
       "         0.8404956 , -0.06019488, -0.07903399,  0.29997196,  0.55319739,\n",
       "         0.60729503, -0.10907525,  0.04783283,  0.33979011,  0.3839887 ,\n",
       "         0.3860405 ,  0.17411641,  0.15875186,  0.03734816,  0.19584133,\n",
       "         0.14867472,  0.25425119,  0.14634071,  0.40984468,  0.51729792,\n",
       "         0.05805776,  0.00122176,  0.8168331 ,  0.08095776,  0.08182919,\n",
       "         0.08083589,  0.3336741 ,  0.36463529,  0.42725213,  0.37953982,\n",
       "         0.01203189,  0.57378817, -0.05040895,  0.47352592,  0.51573575,\n",
       "         0.50237034,  0.43274868,  0.19031785,  0.32825265,  0.37082139,\n",
       "         0.34825692,  0.40705651,  0.83290736,  0.02051509,  0.01094772,\n",
       "         0.18452787,  0.4515847 ,  0.4861225 ,  0.32067403,  0.57773763]))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "# Read data\n",
    "data = pd.read_csv('a4_synthetic.csv')\n",
    "\n",
    "# Convert input and output to numpy arrays\n",
    "X = data.drop(columns='y').to_numpy()\n",
    "Y = data.y.to_numpy()\n",
    "\n",
    "(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31eb904",
   "metadata": {},
   "source": [
    "#### Sanity Check for Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a2eccd0e22967383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T16:40:57.000004400Z",
     "start_time": "2024-02-25T16:40:56.492097800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE = 0.7999662647869263\n",
      "Epoch 2: MSE = 0.017392394159767264\n",
      "Epoch 3: MSE = 0.009377418162580966\n",
      "Epoch 4: MSE = 0.009355327616258364\n",
      "Epoch 5: MSE = 0.009365440349979508\n",
      "Epoch 6: MSE = 0.009366988411857164\n",
      "Epoch 7: MSE = 0.009367207068114567\n",
      "Epoch 8: MSE = 0.009367238481529512\n",
      "Epoch 9: MSE = 0.009367244712136654\n",
      "Epoch 10: MSE = 0.009367244620257224\n"
     ]
    }
   ],
   "source": [
    "# Random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Initialize parameters\n",
    "w_init = np.random.normal(size=(2, 1))\n",
    "b_init = np.random.normal(size=(1, 1))\n",
    "\n",
    "# Declare the parameter tensors\n",
    "w = torch.tensor(w_init, dtype=torch.float32, requires_grad=True)\n",
    "b = torch.tensor(b_init, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Initialize the optimizer\n",
    "eta = 1e-2\n",
    "opt = optim.SGD([w, b], lr=eta)\n",
    "\n",
    "for i in range(10):\n",
    "    # Reset sum_err at the beginning of each epoch\n",
    "    sum_err = 0\n",
    "\n",
    "    for row in range(X.shape[0]):\n",
    "        # Fix the input and output\n",
    "        x = torch.tensor(X[row, :], dtype=torch.float32).view(1, -1)\n",
    "        y = torch.tensor(Y[row], dtype=torch.float32).view(1, -1)\n",
    "\n",
    "        # Forward pass.\n",
    "        y_pred = x.mm(w) + b\n",
    "        err = (y_pred - y).pow(2).sum()\n",
    "\n",
    "        # Backward and update.\n",
    "        opt.zero_grad()\n",
    "        err.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # For statistics.\n",
    "        sum_err += err.item()\n",
    "\n",
    "    mse = sum_err / X.shape[0]\n",
    "    print(f'Epoch {i+1}: MSE =', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91ba3fee8f92ad9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Task 1 Discussion\n",
    "Our model demonstrated substantial improvement over the training period, with the MSE reducing from an initial value of 0.7999 in epoch 1 to approximately 0.0094 by epoch 10. This progression indicates successful parameter optimization, aligning with the expected outcome of a correctly implemented linear regression model in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4539efc4250cf860",
   "metadata": {},
   "source": [
    "### Task 2: Implementing tensor arithmetics for forward computations\n",
    "***\n",
    "\n",
    "We decided to centralize tasks 2-4 in the next cell because we faced several challenges when trying to replicate and adjust the code across multiple cells for each task.  Initially, we copied the code from previous steps to incorporate these new requirements. But this strategy made it harder to keep track of changes and increased the likelihood of mistakes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e580b2c16c6713f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T16:40:57.066053800Z",
     "start_time": "2024-02-25T16:40:57.037048400Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "\n",
    "    # Constructor. Just store the input values.\n",
    "    def __init__(self, data, requires_grad=False, grad_fn=None):\n",
    "        self.data = data\n",
    "        self.shape = data.shape\n",
    "        self.grad_fn = grad_fn\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None\n",
    "\n",
    "    # So that we can print the object or show it in a notebook cell.\n",
    "    def __repr__(self):\n",
    "        dstr = repr(self.data)\n",
    "        if self.requires_grad:\n",
    "            gstr = ', requires_grad=True'\n",
    "        elif self.grad_fn is not None:\n",
    "            gstr = f', grad_fn={self.grad_fn}'\n",
    "        else:\n",
    "            gstr = ''\n",
    "        return f'Tensor({dstr}{gstr})'\n",
    "\n",
    "    # Extract one numerical value from this tensor.\n",
    "    def item(self):\n",
    "        return self.data.item()\n",
    "\n",
    "    # Operator +\n",
    "    def __add__(self, right):\n",
    "        if not isinstance(right, Tensor):\n",
    "            right = tensor(right)\n",
    "        return addition(self, right)\n",
    "\n",
    "    # Operator -\n",
    "    def __sub__(self, right):\n",
    "        if not isinstance(right, Tensor):\n",
    "            right = tensor(right)\n",
    "        return subtraction(self, right)\n",
    "\n",
    "    # Operator @\n",
    "    def __matmul__(self, right):\n",
    "        if not isinstance(right, Tensor):\n",
    "            raise ValueError(\"Right operand must be a Tensor\")\n",
    "        return matrix_multi(self, right)\n",
    "\n",
    "    # Operator **\n",
    "    def __pow__(self, right):\n",
    "        # NOTE! We are assuming that right is an integer here, not a Tensor!\n",
    "        if not isinstance(right, int):\n",
    "            raise Exception('only integers allowed')\n",
    "        if right < 2:\n",
    "            raise Exception('power must be >= 2')\n",
    "        return power(self, right)\n",
    "    \n",
    "    # Operator .sum()\n",
    "    def sum(self):\n",
    "        sum_data = np.sum(self.data)\n",
    "        grad_fn = SummationNode(self) if self.requires_grad else None\n",
    "        return Tensor(sum_data, requires_grad=self.requires_grad, grad_fn=grad_fn)\n",
    "\n",
    "    # Backward computations. Implemented in Task 4.\n",
    "    def backward(self, grad_output=None):\n",
    "        # Skip gradient computation if not required\n",
    "        if not self.requires_grad:\n",
    "            return\n",
    "    \n",
    "        # Use provided gradient or default to ones if none is provided\n",
    "        grad_output = np.ones_like(self.data) if grad_output is None else grad_output\n",
    "    \n",
    "        # Initialize or accumulate gradient\n",
    "        self.grad = grad_output if self.grad is None else self.grad + grad_output\n",
    "    \n",
    "        # Propagate gradient to previous operation if it exists\n",
    "        if self.grad_fn:\n",
    "            self.grad_fn.backward(grad_output)\n",
    "            \n",
    "# A small utility where we simply create a Tensor object\n",
    "def tensor(data, requires_grad=False):\n",
    "    return Tensor(data, requires_grad)\n",
    "\n",
    "# We define helper functions to implement the various arithmetic operations.\n",
    "def addition(left, right):\n",
    "    # Perform element-wise addition\n",
    "    new_data = left.data + right.data\n",
    "\n",
    "    # Determine if a gradient function is needed\n",
    "    if left.requires_grad or right.requires_grad:\n",
    "        grad_fn = AdditionNode(left, right)\n",
    "    else:\n",
    "        grad_fn = None\n",
    "\n",
    "    # Return a new Tensor object\n",
    "    return Tensor(new_data, requires_grad=left.requires_grad or right.requires_grad, grad_fn=grad_fn)\n",
    "\n",
    "def subtraction(left, right):\n",
    "    # Perform element-wise subtraction\n",
    "    new_data = left.data - right.data\n",
    "\n",
    "    # Determine if a gradient function is needed\n",
    "    if left.requires_grad or right.requires_grad:\n",
    "        grad_fn = SubtractionNode(left, right)\n",
    "    else:\n",
    "        grad_fn = None\n",
    "\n",
    "    # Return a new Tensor object\n",
    "    return Tensor(new_data, requires_grad=left.requires_grad or right.requires_grad, grad_fn=grad_fn)\n",
    "\n",
    "def matrix_multi(left, right):\n",
    "    # Check for shape compatibility for matrix multiplication\n",
    "    if left.shape[1] != right.shape[0]:\n",
    "        raise ValueError(\"Shapes are not compatible for matrix multiplication\")\n",
    "\n",
    "    # Perform matrix multiplication\n",
    "    new_data = np.dot(left.data, right.data)\n",
    "\n",
    "    # Determine if a gradient function is needed\n",
    "    if left.requires_grad or right.requires_grad:\n",
    "        grad_fn = MatMulNode(left, right)\n",
    "    else:\n",
    "        grad_fn = None\n",
    "\n",
    "    # Return a new Tensor object\n",
    "    return Tensor(new_data, requires_grad=left.requires_grad or right.requires_grad, grad_fn=grad_fn)\n",
    "\n",
    "def power(tensor, exponent):\n",
    "    # Perform power operation\n",
    "    new_data = tensor.data ** exponent\n",
    "\n",
    "    # Determine if a gradient function is needed\n",
    "    if tensor.requires_grad:\n",
    "        grad_fn = PowerNode(tensor, exponent)\n",
    "    else:\n",
    "        grad_fn = None\n",
    "\n",
    "    # Return a new Tensor object\n",
    "    return Tensor(new_data, requires_grad=tensor.requires_grad, grad_fn=grad_fn)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        raise NotImplementedError('Backward method not implemented.')\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(type(self))\n",
    "\n",
    "class AdditionNode(Node):\n",
    "    # Represents an addition operation in the graph.\n",
    "    def __init__(self, left, right):\n",
    "        super().__init__()\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Propagate gradients back to operands of the addition.\n",
    "        if self.left.requires_grad:\n",
    "            self.left.grad = grad_output if self.left.grad is None else self.left.grad + grad_output\n",
    "            if self.left.grad_fn:\n",
    "                self.left.grad_fn.backward(grad_output)\n",
    "\n",
    "        if self.right.requires_grad:\n",
    "            self.right.grad = grad_output if self.right.grad is None else self.right.grad + grad_output\n",
    "            if self.right.grad_fn:\n",
    "                self.right.grad_fn.backward(grad_output)\n",
    "\n",
    "class SubtractionNode(Node):\n",
    "    # Represents a subtraction operation in the graph.\n",
    "    def __init__(self, left, right):\n",
    "        super().__init__()\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Propagate gradients back to operands, adjusting for the operation's nature.\n",
    "        if self.left.requires_grad:\n",
    "            self.left.grad = grad_output if self.left.grad is None else self.left.grad + grad_output\n",
    "            if self.left.grad_fn:\n",
    "                self.left.grad_fn.backward(grad_output)\n",
    "\n",
    "        if self.right.requires_grad:\n",
    "            # Gradient for subtraction is negated for the right operand.\n",
    "            neg_grad_output = -grad_output\n",
    "            self.right.grad = neg_grad_output if self.right.grad is None else self.right.grad + neg_grad_output\n",
    "            if self.right.grad_fn:\n",
    "                self.right.grad_fn.backward(neg_grad_output)\n",
    "\n",
    "class MatMulNode(Node):\n",
    "    # Represents a matrix multiplication operation in the graph.\n",
    "    def __init__(self, left, right):\n",
    "        super().__init__()\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Propagate gradients according to matrix multiplication rules.\n",
    "        if self.left.requires_grad:\n",
    "            # Gradient w.r.t. the left operand is computed by dotting grad_output with right's transpose.\n",
    "            grad_left = np.dot(grad_output, self.right.data.T)\n",
    "            self.left.grad = grad_left if self.left.grad is None else self.left.grad + grad_left\n",
    "            if self.left.grad_fn:\n",
    "                self.left.grad_fn.backward(grad_left)\n",
    "\n",
    "        if self.right.requires_grad:\n",
    "            # Gradient w.r.t. the right operand involves left's transpose and grad_output.\n",
    "            grad_right = np.dot(self.left.data.T, grad_output)\n",
    "            self.right.grad = grad_right if self.right.grad is None else self.right.grad + grad_right\n",
    "            if self.right.grad_fn:\n",
    "                self.right.grad_fn.backward(grad_right)\n",
    "\n",
    "class PowerNode(Node):\n",
    "    # Represents a power operation in the graph.\n",
    "    def __init__(self, tensor, exponent):\n",
    "        super().__init__()\n",
    "        self.tensor = tensor\n",
    "        self.exponent = exponent\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Propagate gradient based on the power rule.\n",
    "        grad_tensor = self.exponent * (self.tensor.data ** (self.exponent - 1))\n",
    "        self.tensor.grad = grad_output * grad_tensor if self.tensor.grad is None else self.tensor.grad + (grad_output * grad_tensor)\n",
    "        if self.tensor.grad_fn:\n",
    "            self.tensor.grad_fn.backward(grad_output * grad_tensor)\n",
    "\n",
    "class SummationNode(Node):\n",
    "    # Represents a summation operation over all elements in the tensor.\n",
    "    def __init__(self, tensor):\n",
    "        super().__init__()\n",
    "        self.tensor = tensor\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Propagate gradients uniformly to each element of the summed tensor.\n",
    "        if self.tensor.requires_grad:\n",
    "            self.tensor.grad = np.ones_like(self.tensor.data) * grad_output if self.tensor.grad is None else self.tensor.grad + (np.ones_like(self.tensor.data) * grad_output)\n",
    "            if self.tensor.grad_fn:\n",
    "                self.tensor.grad_fn.backward(grad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c364ff78",
   "metadata": {},
   "source": [
    "#### Sanity Check for Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b4a0ef71c0d30a4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T16:40:57.094055Z",
     "start_time": "2024-02-25T16:40:57.067053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test of addition: [[2. 3.]] + [[1. 4.]] = [[3. 7.]]\n",
      "Test of subtraction: [[2. 3.]] - [[1. 4.]] = [[ 1. -1.]]\n",
      "Test of power: [[1. 4.]] ** 2 = [[ 1. 16.]]\n",
      "Test of matrix multiplication: [[2. 3.]] @ [[-1. ]\n",
      " [ 1.2]] = [[1.6]]\n"
     ]
    }
   ],
   "source": [
    "# Two tensors holding row vectors.\n",
    "x1 = tensor(np.array([[2.0, 3.0]]))\n",
    "x2 = tensor(np.array([[1.0, 4.0]]))\n",
    "# A tensors holding a column vector.\n",
    "w = tensor(np.array([[-1.0], [1.2]]))\n",
    "\n",
    "# Test the arithmetic operations.\n",
    "test_plus = x1 + x2\n",
    "test_minus = x1 - x2\n",
    "test_power = x2 ** 2\n",
    "test_matmul = x1 @ w\n",
    "\n",
    "print(f'Test of addition: {x1.data} + {x2.data} = {test_plus.data}')\n",
    "print(f'Test of subtraction: {x1.data} - {x2.data} = {test_minus.data}')\n",
    "print(f'Test of power: {x2.data} ** 2 = {test_power.data}')\n",
    "print(f'Test of matrix multiplication: {x1.data} @ {w.data} = {test_matmul.data}')\n",
    "\n",
    "# Check that the results are as expected. Will crash if there is a miscalculation.\n",
    "assert(np.allclose(test_plus.data, np.array([[3.0, 7.0]])))\n",
    "assert(np.allclose(test_minus.data, np.array([[1.0, -1.0]])))\n",
    "assert(np.allclose(test_power.data, np.array([[1.0, 16.0]])))\n",
    "assert(np.allclose(test_matmul.data, np.array([[1.6]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e613def7599835",
   "metadata": {},
   "source": [
    "### Task 3: Building the computational graph\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa667b",
   "metadata": {},
   "source": [
    "#### Sanity Check for Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "424df74ee3500e56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T16:40:57.160052400Z",
     "start_time": "2024-02-25T16:40:57.101058300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational graph top node after x + w1 + w2: <class '__main__.AdditionNode'>\n"
     ]
    }
   ],
   "source": [
    "x = tensor(np.array([[2.0, 3.0]]))\n",
    "w1 = tensor(np.array([[1.0, 4.0]]), requires_grad=True)\n",
    "w2 = tensor(np.array([[3.0, -1.0]]), requires_grad=True)\n",
    "\n",
    "test_graph = x + w1 + w2\n",
    "\n",
    "print('Computational graph top node after x + w1 + w2:', test_graph.grad_fn)\n",
    "\n",
    "assert(isinstance(test_graph.grad_fn, AdditionNode))\n",
    "assert(test_graph.grad_fn.right is w2)\n",
    "assert(test_graph.grad_fn.left.grad_fn.left is x)\n",
    "assert(test_graph.grad_fn.left.grad_fn.right is w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7db0907890df63",
   "metadata": {},
   "source": [
    "### Task 4: Implementing the backward computations\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76824dc8",
   "metadata": {},
   "source": [
    "#### Sanity Check for Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6d81b29112dc2895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T16:40:57.178054600Z",
     "start_time": "2024-02-25T16:40:57.116052200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of loss w.r.t. w =\n",
      " [[5.6]\n",
      " [8.4]]\n"
     ]
    }
   ],
   "source": [
    "x = tensor(np.array([[2.0, 3.0]]))\n",
    "w = tensor(np.array([[-1.0], [1.2]]), requires_grad=True)\n",
    "y = tensor(np.array([[0.2]]))\n",
    "\n",
    "# We could as well write simply loss = (x @ w - y)**2\n",
    "# We break it down into steps here if you need to debug.\n",
    "\n",
    "model_out = x @ w\n",
    "diff = model_out - y\n",
    "loss = diff ** 2\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('Gradient of loss w.r.t. w =\\n', w.grad)\n",
    "\n",
    "assert(np.allclose(w.grad, np.array([[5.6], [8.4]])))\n",
    "assert(x.grad is None)\n",
    "assert(y.grad is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "718faed6277dd405",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T16:40:57.258052400Z",
     "start_time": "2024-02-25T16:40:57.131059600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.6000],\n",
       "        [8.4000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch Task 4 Sanity check\n",
    "pt_x = torch.tensor(np.array([[2.0, 3.0]]))\n",
    "pt_w = torch.tensor(np.array([[-1.0], [1.2]]), requires_grad=True)\n",
    "pt_y = torch.tensor(np.array([[0.2]]))\n",
    "\n",
    "pt_model_out = pt_x @ pt_w\n",
    "pt_model_out.retain_grad() # Keep the gradient of intermediate nodes for debugging.\n",
    "\n",
    "pt_diff = pt_model_out - pt_y\n",
    "pt_diff.retain_grad()\n",
    "\n",
    "pt_loss = pt_diff ** 2\n",
    "pt_loss.retain_grad()\n",
    "\n",
    "pt_loss.backward()\n",
    "pt_w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb06c52184da60",
   "metadata": {},
   "source": [
    "### Task 5: Optimizers to update the model parameters\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "62291d50d9a3c63d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T16:40:57.277047800Z",
     "start_time": "2024-02-25T16:40:57.174053700Z"
    }
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplementedError('Step method not implemented.')\n",
    "    \n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, lr):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        # Update the parameters using the gradients\n",
    "        for p in self.params:\n",
    "            p.data -= self.lr * p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd630d94",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "05d29dbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T16:40:57.353051300Z",
     "start_time": "2024-02-25T16:40:57.327048500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Class to hold the parameters and gradients\n",
    "class Parameter:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.grad = np.zeros_like(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cf9920ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T16:40:57.462048600Z",
     "start_time": "2024-02-25T16:40:57.347056100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE = 0.7999661130823179\n",
      "Epoch 2: MSE = 0.017392390107906875\n",
      "Epoch 3: MSE = 0.009377418010839892\n",
      "Epoch 4: MSE = 0.009355326971438458\n",
      "Epoch 5: MSE = 0.009365440968904258\n",
      "Epoch 6: MSE = 0.009366989180952535\n",
      "Epoch 7: MSE = 0.009367207398577987\n",
      "Epoch 8: MSE = 0.00936723898397449\n",
      "Epoch 9: MSE = 0.009367243704122534\n",
      "Epoch 10: MSE = 0.009367244427185761\n"
     ]
    }
   ],
   "source": [
    "# Random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Initialize parameters\n",
    "w_init = np.random.normal(size=(2, 1))\n",
    "b_init = np.random.normal(size=(1, 1))\n",
    "\n",
    "# Create parameter objects\n",
    "w = Parameter(w_init)\n",
    "b = Parameter(b_init)\n",
    "\n",
    "# Initialize the optimizer\n",
    "params = [w, b]\n",
    "eta = 1e-2\n",
    "opt = SGD(params, lr=eta)\n",
    "\n",
    "# Training loop\n",
    "for i in range(10):\n",
    "    sum_err = 0\n",
    "    for row in range(X.shape[0]):\n",
    "        # Reset gradients at the beginning of each step\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # Fix the input and output\n",
    "        x = X[row, :].reshape(1, -1)\n",
    "        y = Y[row].reshape(1, -1)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = x @ w.data + b.data\n",
    "        err = ((y_pred - y) ** 2).sum()\n",
    "\n",
    "        # Compute gradients\n",
    "        err_grad = 2 * (y_pred - y)\n",
    "        w.grad = x.T @ err_grad\n",
    "        b.grad = np.sum(err_grad, axis=0, keepdims=True)\n",
    "\n",
    "        # Update parameters\n",
    "        opt.step()\n",
    "\n",
    "        # Accumulate error\n",
    "        sum_err += err\n",
    "\n",
    "    mse = sum_err / X.shape[0]\n",
    "    print(f'Epoch {i+1}: MSE =', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef0c44",
   "metadata": {},
   "source": [
    "#### Summary Table MSE Task 1 and Task 5\n",
    "\n",
    "| Epoch | MSE Task 1         | MSE Task 5         |\n",
    "|-------|--------------------|--------------------|\n",
    "| 1     | 0.7999662647869263 | 0.7999661130823179 |\n",
    "| 2     | 0.017392394159767264 | 0.017392390107906875 |\n",
    "| 3     | 0.009377418162580966 | 0.009377418010839892 |\n",
    "| 4     | 0.009355327616258364 | 0.009355326971438458 |\n",
    "| 5     | 0.009365440349979508 | 0.009365440968904258 |\n",
    "| 6     | 0.009366988411857164 | 0.009366989180952535 |\n",
    "| 7     | 0.009367207068114567 | 0.009367207398577987 |\n",
    "| 8     | 0.009367238481529512 | 0.00936723898397449  |\n",
    "| 9     | 0.009367244712136654 | 0.009367243704122534 |\n",
    "| 10    | 0.009367244620257224 | 0.009367244427185761 |\n",
    "\n",
    "The MSE values are almost identical, indicating that the optimizer is functioning as expected. The optimizer is updating the model parameters in a way that is consistent with the expected outcome of a correctly implemented linear regression model in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61112bebd20cf885",
   "metadata": {},
   "source": [
    "### Task 6: Classifying raisins\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f68f5f2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T16:40:57.898047900Z",
     "start_time": "2024-02-25T16:40:57.442047200Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the raisins dataset\n",
    "a4data = pd.read_csv('raisins.csv')\n",
    "\n",
    "# Scale the input data\n",
    "X = scale(a4data.drop(columns='Class'))\n",
    "Y = 1.0*(a4data.Class == 'Besni').to_numpy()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(len(Y))\n",
    "X = X[shuffle]\n",
    "Y = Y[shuffle]\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, random_state=0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1b62498b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T16:40:57.917047900Z",
     "start_time": "2024-02-25T16:40:57.903049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((720, 7), (720,), (180, 7), (180,))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, Ytrain.shape, Xtest.shape, Ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b57af153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T16:40:57.987045400Z",
     "start_time": "2024-02-25T16:40:57.919047600Z"
    }
   },
   "outputs": [],
   "source": [
    "Ytrain = Ytrain.reshape(-1, 1)\n",
    "Ytest = Ytest.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2e2d0252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T16:40:58.024040800Z",
     "start_time": "2024-02-25T16:40:57.945061700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tanh test: [ 0.         -0.96402758]\n",
      "Tanh derivative test: [1.         0.07065082]\n",
      "Sigmoid test: [0.5        0.11920292]\n",
      "Sigmoid derivative test: [0.25       0.10499359]\n",
      "Binary Cross-Entropy Loss test: 0.10536051565782628\n",
      "Binary Cross-Entropy Derivative test: [-1.11111111  1.11111111]\n"
     ]
    }
   ],
   "source": [
    "# Activation functions and their derivatives\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Binary Cross-Entropy Loss and its derivative\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    # Applying epsilon and clipping to prevent log(0)\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_cross_entropy_derivative(y_true, y_pred):\n",
    "    epsilon = 1e-15  # to prevent division by zero\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clipping predictions to avoid division by zero\n",
    "    return -(y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "\n",
    "# Test functions\n",
    "print(\"Tanh test:\", tanh(np.array([0, -2])))\n",
    "print(\"Tanh derivative test:\", tanh_derivative(np.array([0, -2])))\n",
    "print(\"Sigmoid test:\", sigmoid(np.array([0, -2])))\n",
    "print(\"Sigmoid derivative test:\", sigmoid_derivative(np.array([0, -2])))\n",
    "print(\"Binary Cross-Entropy Loss test:\", binary_cross_entropy(np.array([1, 0]), np.array([0.9, 0.1])))\n",
    "print(\"Binary Cross-Entropy Derivative test:\", binary_cross_entropy_derivative(np.array([1, 0]), np.array([0.9, 0.1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a1439517fd78b1de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T16:40:58.073040300Z",
     "start_time": "2024-02-25T16:40:57.979043300Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        # Initialize weights and biases for the first (input to hidden) layer\n",
    "        self.W1 = np.random.randn(input_size, hidden_size)\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        \n",
    "        # Initialize weights and biases for the second (hidden to output) layer\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = np.zeros(output_size)\n",
    "        \n",
    "        # Set the learning rate for the training\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Forward pass through the first layer\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1  # Linear step\n",
    "        self.A1 = tanh(self.Z1)  # Activation step\n",
    "        \n",
    "        # Forward pass through the second layer\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2  # Linear step\n",
    "        self.A2 = 1 / (1 + np.exp(-self.Z2))  # Sigmoid activation for binary classification output\n",
    "        \n",
    "        return self.A2  # Return the final output\n",
    "    \n",
    "    def compute_loss(self, Y, Y_hat):\n",
    "        # Compute the binary cross-entropy loss\n",
    "        return binary_cross_entropy(Y, Y_hat)\n",
    "    \n",
    "    def backward(self, X, Y):\n",
    "        # Backpropagation to update weights and biases\n",
    "        \n",
    "        # Compute derivative of loss w.r.t. output activation\n",
    "        dA2 = binary_cross_entropy_derivative(Y, self.A2)\n",
    "        \n",
    "        # Derivative of the loss w.r.t. Z2, then update W2 and b2\n",
    "        dZ2 = dA2 * (self.A2 * (1 - self.A2))  # Sigmoid derivative\n",
    "        dW2 = np.dot(self.A1.T, dZ2)\n",
    "        db2 = np.sum(dZ2, axis=0)\n",
    "        \n",
    "        # Derivative of the loss w.r.t. Z1, then update W1 and b1\n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        dZ1 = dA1 * tanh_derivative(self.Z1)  # tanh derivative\n",
    "        dW1 = np.dot(X.T, dZ1)\n",
    "        db1 = np.sum(dZ1, axis=0)\n",
    "        \n",
    "        # Update the weights and biases, applying the learning rate\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "    \n",
    "    def train(self, X_train, Y_train, epochs=100):\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass: compute predicted output\n",
    "            Y_hat = self.forward(X_train)\n",
    "            \n",
    "            # Compute and print loss and accuracy every 100 epochs\n",
    "            loss = self.compute_loss(Y_train, Y_hat)\n",
    "            self.backward(X_train, Y_train)  # Backward pass to update weights\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                predictions = self.forward(X_train) >= 0.5  # Threshold predictions to binary outcomes\n",
    "                accuracy = np.mean(predictions == Y_train)  # Calculate accuracy\n",
    "                print(f'Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "93023dcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T16:40:58.934475700Z",
     "start_time": "2024-02-25T16:40:57.996049200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.8510, Accuracy: 0.8472\n",
      "Epoch 100, Loss: 0.3498, Accuracy: 0.8708\n",
      "Epoch 200, Loss: 0.3376, Accuracy: 0.8764\n",
      "Epoch 300, Loss: 0.3312, Accuracy: 0.8778\n",
      "Epoch 400, Loss: 0.3221, Accuracy: 0.8806\n",
      "Epoch 500, Loss: 0.3157, Accuracy: 0.8847\n",
      "Epoch 600, Loss: 0.3128, Accuracy: 0.8833\n",
      "Epoch 700, Loss: 0.3095, Accuracy: 0.8833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 800, Loss: 0.3066, Accuracy: 0.8847\n",
      "Epoch 900, Loss: 0.3048, Accuracy: 0.8833\n",
      "Epoch 1000, Loss: 0.3015, Accuracy: 0.8792\n",
      "Epoch 1100, Loss: 0.2969, Accuracy: 0.8806\n",
      "Epoch 1200, Loss: 0.2921, Accuracy: 0.8806\n",
      "Epoch 1300, Loss: 0.2876, Accuracy: 0.8847\n",
      "Epoch 1400, Loss: 0.2836, Accuracy: 0.8861\n",
      "Epoch 1500, Loss: 0.2801, Accuracy: 0.8861\n",
      "Epoch 1600, Loss: 0.2769, Accuracy: 0.8903\n",
      "Epoch 1700, Loss: 0.2739, Accuracy: 0.8903\n",
      "Epoch 1800, Loss: 0.2712, Accuracy: 0.8903\n",
      "Epoch 1900, Loss: 0.2690, Accuracy: 0.8903\n",
      "Epoch 2000, Loss: 0.2670, Accuracy: 0.8917\n",
      "Epoch 2100, Loss: 0.2653, Accuracy: 0.8903\n",
      "Epoch 2200, Loss: 0.2638, Accuracy: 0.8889\n",
      "Epoch 2300, Loss: 0.2625, Accuracy: 0.8931\n",
      "Epoch 2400, Loss: 0.2613, Accuracy: 0.8944\n",
      "Epoch 2500, Loss: 0.2602, Accuracy: 0.8972\n",
      "Epoch 2600, Loss: 0.2592, Accuracy: 0.8958\n",
      "Epoch 2700, Loss: 0.2583, Accuracy: 0.8958\n",
      "Epoch 2800, Loss: 0.2575, Accuracy: 0.8958\n",
      "Epoch 2900, Loss: 0.2567, Accuracy: 0.8958\n",
      "Epoch 3000, Loss: 0.2560, Accuracy: 0.8958\n",
      "Epoch 3100, Loss: 0.2554, Accuracy: 0.8958\n",
      "Epoch 3200, Loss: 0.2547, Accuracy: 0.8972\n",
      "Epoch 3300, Loss: 0.2542, Accuracy: 0.8958\n",
      "Epoch 3400, Loss: 0.2536, Accuracy: 0.8958\n",
      "Epoch 3500, Loss: 0.2531, Accuracy: 0.8958\n",
      "Epoch 3600, Loss: 0.2526, Accuracy: 0.8958\n",
      "Epoch 3700, Loss: 0.2521, Accuracy: 0.8958\n",
      "Epoch 3800, Loss: 0.2517, Accuracy: 0.8958\n",
      "Epoch 3900, Loss: 0.2512, Accuracy: 0.8958\n",
      "Epoch 4000, Loss: 0.2508, Accuracy: 0.8972\n",
      "Epoch 4100, Loss: 0.2504, Accuracy: 0.8972\n",
      "Epoch 4200, Loss: 0.2500, Accuracy: 0.8972\n",
      "Epoch 4300, Loss: 0.2497, Accuracy: 0.8986\n",
      "Epoch 4400, Loss: 0.2493, Accuracy: 0.8986\n",
      "Epoch 4500, Loss: 0.2490, Accuracy: 0.8986\n",
      "Epoch 4600, Loss: 0.2486, Accuracy: 0.8986\n",
      "Epoch 4700, Loss: 0.2483, Accuracy: 0.8986\n",
      "Epoch 4800, Loss: 0.2480, Accuracy: 0.8986\n",
      "Epoch 4900, Loss: 0.2477, Accuracy: 0.8986\n",
      "Epoch 5000, Loss: 0.2474, Accuracy: 0.9000\n",
      "Epoch 5100, Loss: 0.2471, Accuracy: 0.9000\n",
      "Epoch 5200, Loss: 0.2469, Accuracy: 0.9000\n",
      "Epoch 5300, Loss: 0.2466, Accuracy: 0.9000\n",
      "Epoch 5400, Loss: 0.2463, Accuracy: 0.9000\n",
      "Epoch 5500, Loss: 0.2461, Accuracy: 0.9000\n",
      "Epoch 5600, Loss: 0.2459, Accuracy: 0.9000\n",
      "Epoch 5700, Loss: 0.2456, Accuracy: 0.9000\n",
      "Epoch 5800, Loss: 0.2454, Accuracy: 0.8986\n",
      "Epoch 5900, Loss: 0.2452, Accuracy: 0.8986\n",
      "Epoch 6000, Loss: 0.2450, Accuracy: 0.8986\n",
      "Epoch 6100, Loss: 0.2448, Accuracy: 0.8986\n",
      "Epoch 6200, Loss: 0.2446, Accuracy: 0.8986\n",
      "Epoch 6300, Loss: 0.2444, Accuracy: 0.8986\n",
      "Epoch 6400, Loss: 0.2442, Accuracy: 0.8986\n",
      "Epoch 6500, Loss: 0.2440, Accuracy: 0.9014\n",
      "Epoch 6600, Loss: 0.2438, Accuracy: 0.9014\n",
      "Epoch 6700, Loss: 0.2436, Accuracy: 0.9028\n",
      "Epoch 6800, Loss: 0.2435, Accuracy: 0.9042\n",
      "Epoch 6900, Loss: 0.2434, Accuracy: 0.9042\n",
      "Epoch 7000, Loss: 0.2433, Accuracy: 0.9042\n",
      "Epoch 7100, Loss: 0.2432, Accuracy: 0.9042\n",
      "Epoch 7200, Loss: 0.2431, Accuracy: 0.9042\n",
      "Epoch 7300, Loss: 0.2429, Accuracy: 0.9056\n",
      "Epoch 7400, Loss: 0.2427, Accuracy: 0.9056\n",
      "Epoch 7500, Loss: 0.2425, Accuracy: 0.9056\n",
      "Epoch 7600, Loss: 0.2423, Accuracy: 0.9069\n",
      "Epoch 7700, Loss: 0.2420, Accuracy: 0.9069\n",
      "Epoch 7800, Loss: 0.2417, Accuracy: 0.9069\n",
      "Epoch 7900, Loss: 0.2414, Accuracy: 0.9069\n",
      "Epoch 8000, Loss: 0.2411, Accuracy: 0.9069\n",
      "Epoch 8100, Loss: 0.2408, Accuracy: 0.9069\n",
      "Epoch 8200, Loss: 0.2405, Accuracy: 0.9069\n",
      "Epoch 8300, Loss: 0.2402, Accuracy: 0.9069\n",
      "Epoch 8400, Loss: 0.2399, Accuracy: 0.9069\n",
      "Epoch 8500, Loss: 0.2396, Accuracy: 0.9069\n",
      "Epoch 8600, Loss: 0.2394, Accuracy: 0.9069\n",
      "Epoch 8700, Loss: 0.2391, Accuracy: 0.9069\n",
      "Epoch 8800, Loss: 0.2388, Accuracy: 0.9069\n",
      "Epoch 8900, Loss: 0.2385, Accuracy: 0.9069\n",
      "Epoch 9000, Loss: 0.2383, Accuracy: 0.9069\n",
      "Epoch 9100, Loss: 0.2380, Accuracy: 0.9069\n",
      "Epoch 9200, Loss: 0.2377, Accuracy: 0.9069\n",
      "Epoch 9300, Loss: 0.2375, Accuracy: 0.9069\n",
      "Epoch 9400, Loss: 0.2372, Accuracy: 0.9069\n",
      "Epoch 9500, Loss: 0.2370, Accuracy: 0.9069\n",
      "Epoch 9600, Loss: 0.2368, Accuracy: 0.9069\n",
      "Epoch 9700, Loss: 0.2365, Accuracy: 0.9069\n",
      "Epoch 9800, Loss: 0.2363, Accuracy: 0.9069\n",
      "Epoch 9900, Loss: 0.2361, Accuracy: 0.9069\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = Xtrain.shape[1]\n",
    "hidden_size = 10 \n",
    "output_size = 1  # Binary classification\n",
    "learning_rate = 0.01\n",
    "epochs = 10000\n",
    "\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
    "nn.train(Xtrain, Ytrain, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88d785a",
   "metadata": {},
   "source": [
    "The output of the model shows a solid a consistent improvement over the training period, both in terms of decreasing loss and increasing accuracy, a good indicator of a well-performing model. The neural network is effectively learning from the data and is becoming progressively better at classifying raisins.\n",
    "\n",
    "The training starts with a relatively high accuracy and a moderate loss, which indicates that the initial random weights were effective for this task. The initial accuracy at epoch 0 suggest that even before any training, the structure of the model process the data in a meaninful way. The loss and accuracy show consistent improvement over the epochs. This gradual improvement is a sign of effective learning and optimization. \n",
    "\n",
    "The model achieves an accuracy plateau of around 0.9069 after epoch 7600, with some minimal fluctuations from epoch 5000 to 7200. The stability of the accuracy may sugggests that the model has reached a point that is close ot its maximum potential given the current architecture. In order to see additional improvements, it might be required require changes in the architecture of the model or hyperparameter (e.g., learning rate, epochs, sizes of the hidden layer etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bc91d249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.8556\n"
     ]
    }
   ],
   "source": [
    "def predict(nn, X):\n",
    "    \"\"\"Generate predictions using the forward pass of the neural network.\"\"\"\n",
    "    predictions = nn.forward(X)\n",
    "    return predictions >= 0.5  # Threshold predictions to get binary output\n",
    "\n",
    "# Generate predictions for the test set\n",
    "predictions = predict(nn, Xtest)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == Ytest)\n",
    "\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e057b237",
   "metadata": {},
   "source": [
    "The accuracy on the test set is 0.8556. This is a good indicator that the model is generalizing well to new data. The test accuracy is lower than the training accuracy, which is expected. The model is trained on the training data, so it is expected to perform better on this data. The test accuracy is still high, which is a good indicator that the model is generalizing well to new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
