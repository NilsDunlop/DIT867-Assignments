{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e02c6b96b85df5a1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Assignment 4\n",
    "## Group Members:\n",
    "### Nils Dunlop, e-mail: gusdunlni@student.gu.se\n",
    "### Francisco Alejandro Erazo Piza, e-mail: guserafr@student.gu.se\n",
    "### Chukwudumebi Ubogu, e-mail: gusuboch@student.gu.se\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d86d47ab6dab10b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 1: A small linear regression example in PyTorch\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c27312703ea6f271",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.76405235,  0.40015721],\n",
       "        [ 0.97873798,  2.2408932 ],\n",
       "        [ 1.86755799, -0.97727788],\n",
       "        [ 0.95008842, -0.15135721],\n",
       "        [-0.10321885,  0.4105985 ],\n",
       "        [ 0.14404357,  1.45427351],\n",
       "        [ 0.76103773,  0.12167502],\n",
       "        [ 0.44386323,  0.33367433],\n",
       "        [ 1.49407907, -0.20515826],\n",
       "        [ 0.3130677 , -0.85409574],\n",
       "        [-2.55298982,  0.6536186 ],\n",
       "        [ 0.8644362 , -0.74216502],\n",
       "        [ 2.26975462, -1.45436567],\n",
       "        [ 0.04575852, -0.18718385],\n",
       "        [ 1.53277921,  1.46935877],\n",
       "        [ 0.15494743,  0.37816252],\n",
       "        [-0.88778575, -1.98079647],\n",
       "        [-0.34791215,  0.15634897],\n",
       "        [ 1.23029068,  1.20237985],\n",
       "        [-0.38732682, -0.30230275],\n",
       "        [-1.04855297, -1.42001794],\n",
       "        [-1.70627019,  1.9507754 ],\n",
       "        [-0.50965218, -0.4380743 ],\n",
       "        [-1.25279536,  0.77749036],\n",
       "        [-1.61389785, -0.21274028],\n",
       "        [-0.89546656,  0.3869025 ],\n",
       "        [-0.51080514, -1.18063218],\n",
       "        [-0.02818223,  0.42833187],\n",
       "        [ 0.06651722,  0.3024719 ],\n",
       "        [-0.63432209, -0.36274117],\n",
       "        [-0.67246045, -0.35955316],\n",
       "        [-0.81314628, -1.7262826 ],\n",
       "        [ 0.17742614, -0.40178094],\n",
       "        [-1.63019835,  0.46278226],\n",
       "        [-0.90729836,  0.0519454 ],\n",
       "        [ 0.72909056,  0.12898291],\n",
       "        [ 1.13940068, -1.23482582],\n",
       "        [ 0.40234164, -0.68481009],\n",
       "        [-0.87079715, -0.57884966],\n",
       "        [-0.31155253,  0.05616534],\n",
       "        [-1.16514984,  0.90082649],\n",
       "        [ 0.46566244, -1.53624369],\n",
       "        [ 1.48825219,  1.89588918],\n",
       "        [ 1.17877957, -0.17992484],\n",
       "        [-1.07075262,  1.05445173],\n",
       "        [-0.40317695,  1.22244507],\n",
       "        [ 0.20827498,  0.97663904],\n",
       "        [ 0.3563664 ,  0.70657317],\n",
       "        [ 0.01050002,  1.78587049],\n",
       "        [ 0.12691209,  0.40198936],\n",
       "        [ 1.8831507 , -1.34775906],\n",
       "        [-1.270485  ,  0.96939671],\n",
       "        [-1.17312341,  1.94362119],\n",
       "        [-0.41361898, -0.74745481],\n",
       "        [ 1.92294203,  1.48051479],\n",
       "        [ 1.86755896,  0.90604466],\n",
       "        [-0.86122569,  1.91006495],\n",
       "        [-0.26800337,  0.8024564 ],\n",
       "        [ 0.94725197, -0.15501009],\n",
       "        [ 0.61407937,  0.92220667],\n",
       "        [ 0.37642553, -1.09940079],\n",
       "        [ 0.29823817,  1.3263859 ],\n",
       "        [-0.69456786, -0.14963454],\n",
       "        [-0.43515355,  1.84926373],\n",
       "        [ 0.67229476,  0.40746184],\n",
       "        [-0.76991607,  0.53924919],\n",
       "        [-0.67433266,  0.03183056],\n",
       "        [-0.63584608,  0.67643329],\n",
       "        [ 0.57659082, -0.20829876],\n",
       "        [ 0.39600671, -1.09306151],\n",
       "        [-1.49125759,  0.4393917 ],\n",
       "        [ 0.1666735 ,  0.63503144],\n",
       "        [ 2.38314477,  0.94447949],\n",
       "        [-0.91282223,  1.11701629],\n",
       "        [-1.31590741, -0.4615846 ],\n",
       "        [-0.06824161,  1.71334272],\n",
       "        [-0.74475482, -0.82643854],\n",
       "        [-0.09845252, -0.66347829],\n",
       "        [ 1.12663592, -1.07993151],\n",
       "        [-1.14746865, -0.43782004],\n",
       "        [-0.49803245,  1.92953205],\n",
       "        [ 0.94942081,  0.08755124],\n",
       "        [-1.22543552,  0.84436298],\n",
       "        [-1.00021535, -1.5447711 ],\n",
       "        [ 1.18802979,  0.31694261],\n",
       "        [ 0.92085882,  0.31872765],\n",
       "        [ 0.85683061, -0.65102559],\n",
       "        [-1.03424284,  0.68159452],\n",
       "        [-0.80340966, -0.68954978],\n",
       "        [-0.4555325 ,  0.01747916],\n",
       "        [-0.35399391, -1.37495129],\n",
       "        [-0.6436184 , -2.22340315],\n",
       "        [ 0.62523145, -1.60205766],\n",
       "        [-1.10438334,  0.05216508],\n",
       "        [-0.739563  ,  1.5430146 ],\n",
       "        [-1.29285691,  0.26705087],\n",
       "        [-0.03928282, -1.1680935 ],\n",
       "        [ 0.52327666, -0.17154633],\n",
       "        [ 0.77179055,  0.82350415],\n",
       "        [ 2.16323595,  1.33652795]]),\n",
       " array([ 0.53895342,  0.21594929,  0.85495138,  0.5636907 ,  0.32130011,\n",
       "         0.02282172,  0.43355955,  0.28494948,  0.62455331,  0.457357  ,\n",
       "        -0.09488842,  0.58031413,  0.94447131,  0.30172726,  0.32449838,\n",
       "         0.26373822,  0.38526199,  0.28157172,  0.62281853,  0.28318203,\n",
       "         0.20235347, -0.2192494 ,  0.2351877 ,  0.069649  , -0.0850409 ,\n",
       "         0.13153761,  0.37868332,  0.29131062,  0.23830441,  0.2282746 ,\n",
       "         0.10282648,  0.32232423,  0.3400722 ,  0.03356524,  0.04442704,\n",
       "         0.50787709,  0.79897741,  0.25672869,  0.27826382,  0.32854869,\n",
       "        -0.04091009,  0.52842224,  0.35116701,  0.51040517, -0.00891893,\n",
       "        -0.05060942,  0.36320961,  0.4108211 ,  0.04400282,  0.15071379,\n",
       "         0.8404956 , -0.06019488, -0.07903399,  0.29997196,  0.55319739,\n",
       "         0.60729503, -0.10907525,  0.04783283,  0.33979011,  0.3839887 ,\n",
       "         0.3860405 ,  0.17411641,  0.15875186,  0.03734816,  0.19584133,\n",
       "         0.14867472,  0.25425119,  0.14634071,  0.40984468,  0.51729792,\n",
       "         0.05805776,  0.00122176,  0.8168331 ,  0.08095776,  0.08182919,\n",
       "         0.08083589,  0.3336741 ,  0.36463529,  0.42725213,  0.37953982,\n",
       "         0.01203189,  0.57378817, -0.05040895,  0.47352592,  0.51573575,\n",
       "         0.50237034,  0.43274868,  0.19031785,  0.32825265,  0.37082139,\n",
       "         0.34825692,  0.40705651,  0.83290736,  0.02051509,  0.01094772,\n",
       "         0.18452787,  0.4515847 ,  0.4861225 ,  0.32067403,  0.57773763]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "data = pd.read_csv('a4_synthetic.csv')\n",
    "\n",
    "X = data.drop(columns='y').to_numpy()\n",
    "Y = data.y.to_numpy()\n",
    "\n",
    "(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE = 0.7999662647869263\n",
      "Epoch 2: MSE = 0.017392394159767264\n",
      "Epoch 3: MSE = 0.009377418162580966\n",
      "Epoch 4: MSE = 0.009355327616258364\n",
      "Epoch 5: MSE = 0.009365440349979508\n",
      "Epoch 6: MSE = 0.009366988411857164\n",
      "Epoch 7: MSE = 0.009367207068114567\n",
      "Epoch 8: MSE = 0.009367238481529512\n",
      "Epoch 9: MSE = 0.009367244712136654\n",
      "Epoch 10: MSE = 0.009367244620257224\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "w_init = np.random.normal(size=(2, 1))\n",
    "b_init = np.random.normal(size=(1, 1))\n",
    "\n",
    "# We just declare the parameter tensors\n",
    "w = torch.tensor(w_init, dtype=torch.float32, requires_grad=True)\n",
    "b = torch.tensor(b_init, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "eta = 1e-2\n",
    "opt = optim.SGD([w, b], lr=eta)\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    sum_err = 0\n",
    "\n",
    "    for row in range(X.shape[0]):\n",
    "        x = torch.tensor(X[row, :], dtype=torch.float32).view(1, -1)\n",
    "        y = torch.tensor(Y[row], dtype=torch.float32).view(1, -1)\n",
    "\n",
    "        # Forward pass.\n",
    "        y_pred = x.mm(w) + b\n",
    "        err = (y_pred - y).pow(2).sum()\n",
    "\n",
    "        # Backward and update.\n",
    "        opt.zero_grad()\n",
    "        err.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # For statistics.\n",
    "        sum_err += err.item()\n",
    "\n",
    "    mse = sum_err / X.shape[0]\n",
    "    print(f'Epoch {i+1}: MSE =', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Implementing tensor arithmetics for forward computations\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "\n",
    "    # Constructor. Just store the input values.\n",
    "    def __init__(self, data, requires_grad=False, grad_fn=None):\n",
    "        self.data = data\n",
    "        self.shape = data.shape\n",
    "        self.grad_fn = grad_fn\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None\n",
    "\n",
    "    # So that we can print the object or show it in a notebook cell.\n",
    "    def __repr__(self):\n",
    "        dstr = repr(self.data)\n",
    "        if self.requires_grad:\n",
    "            gstr = ', requires_grad=True'\n",
    "        elif self.grad_fn is not None:\n",
    "            gstr = f', grad_fn={self.grad_fn}'\n",
    "        else:\n",
    "            gstr = ''\n",
    "        return f'Tensor({dstr}{gstr})'\n",
    "    \n",
    "    # Reshape tensor values\n",
    "    def reshape(self, *shape):\n",
    "        new_data = self.data.reshape(shape)\n",
    "        return Tensor(new_data, self.requires_grad, self.grad_fn)\n",
    "\n",
    "    # Extract one numerical value from this tensor.\n",
    "    def item(self):\n",
    "        return self.data.item()\n",
    "\n",
    "    # Operator +\n",
    "    def __add__(self, right):\n",
    "        # Call the helper function defined below.\n",
    "        return addition(self, right)\n",
    "\n",
    "    # Operator -\n",
    "    def __sub__(self, right):\n",
    "        return substraction(self, right)\n",
    "\n",
    "    # Operator @\n",
    "    def __matmul__(self, right):\n",
    "        return matmul(self, right)\n",
    "    \n",
    "    # Operator **\n",
    "    def __pow__(self, right):\n",
    "        # NOTE! We are assuming that right is an integer here, not a Tensor!\n",
    "        if not isinstance(right, int):\n",
    "            raise Exception('only integers allowed')\n",
    "        if right < 2:\n",
    "            raise Exception('power must be >= 2')\n",
    "        return power(self, right)\n",
    "\n",
    "    # Backward computations\n",
    "    def backward(self, grad_output=None):\n",
    "        if self.requires_grad:\n",
    "            # Initialize grad_output with ones if this is the loss tensor\n",
    "            if grad_output is None:\n",
    "                grad_output = np.ones_like(self.data)\n",
    "                \n",
    "            # Collect the gradient in grad attribute\n",
    "            if self.grad is None:\n",
    "                self.grad = grad_output\n",
    "            else:\n",
    "                self.grad += grad_output\n",
    "            \n",
    "            # If there's a grad_fn call its backward method\n",
    "            if self.grad_fn is not None:\n",
    "                # Iterate over the inputs to the operation that produced this tensor\n",
    "                for input_tensor in self.grad_fn.inputs:\n",
    "                    if input_tensor.requires_grad:\n",
    "                        grad_wrt_input = grad_output\n",
    "                        input_tensor.backward(grad_wrt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small utility where we simply create a Tensor object. We use this to\n",
    "# mimic torch.tensor.\n",
    "def tensor(data, requires_grad=False):\n",
    "    return Tensor(data, requires_grad)\n",
    "\n",
    "# We define helper functions to implement the various arithmetic operations.\n",
    "\n",
    "# This function takes two tensors as input, and returns a new tensor holding\n",
    "# the result of an element-wise addition on the two input tensors.\n",
    "def addition(left, right):\n",
    "    new_data = left.data + right.data\n",
    "    grad_fn = None\n",
    "    return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "def substraction(left, right):\n",
    "    new_data = left.data - right.data\n",
    "    grad_fn = None\n",
    "    return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "def matmul(left, right):\n",
    "    new_data = left.data.dot(right.data)\n",
    "    grad_fn = None\n",
    "    return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "def power(left, exponent):\n",
    "    new_data = left.data ** exponent\n",
    "    grad_fn = None\n",
    "    return Tensor(new_data, grad_fn=grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition Result: Tensor(array([5, 7, 9]))\n",
      "Subtraction Result: Tensor(array([3, 3, 3]))\n",
      "Matrix Multiplication Result: Tensor(array([[ 8],\n",
      "       [18]]))\n",
      "Power Result: Tensor(array([[ 1,  4],\n",
      "       [ 9, 16]]))\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "# Create some tensors\n",
    "a = tensor(np.array([1, 2, 3]), requires_grad=True)\n",
    "b = tensor(np.array([4, 5, 6]), requires_grad=True)\n",
    "c = tensor(np.array([[1, 2], [3, 4]]), requires_grad=True)\n",
    "d = tensor(np.array([[2], [3]]), requires_grad=True)\n",
    "\n",
    "sum_result = a + b\n",
    "sub_result = b - a\n",
    "matmul_result = c @ d\n",
    "power_result = c ** 2\n",
    "\n",
    "# Print results\n",
    "print(\"Addition Result:\", sum_result)\n",
    "print(\"Subtraction Result:\", sub_result)\n",
    "print(\"Matrix Multiplication Result:\", matmul_result)\n",
    "print(\"Power Result:\", power_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Building the computational graph\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, *inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        raise NotImplementedError('Unimplemented')\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(id={id(self)})\"\n",
    "\n",
    "class AdditionNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        super().__init__(left, right)\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # Placeholder for backward logic\n",
    "        self.inputs[0].grad += 1 * self.inputs.grad_fn\n",
    "        self.inputs[1].grad += 1 * self.inputs.grad_fn \n",
    "        \n",
    "\n",
    "class SubtractionNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        super().__init__(left, right)\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # Placeholder for backward logic\n",
    "        raise NotImplementedError('backward method for SubtractionNode not implemented')\n",
    "    \n",
    "class MatMulNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        super().__init__(left, right)\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Placeholder for backward logic\n",
    "        raise NotImplementedError('backward method for MatMulNode not implemented')\n",
    "    \n",
    "class PowerNode(Node):\n",
    "    def __init__(self, base, exponent):\n",
    "        super().__init__(base)\n",
    "        self.base = base\n",
    "        self.exponent = exponent\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Placeholder for backward logic\n",
    "        raise NotImplementedError('backward method for PowerNode not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tensor object\n",
    "def tensor(data, requires_grad=False):\n",
    "    return Tensor(data, requires_grad)\n",
    "\n",
    "# Update the helper functions to implement the various arithmetic operations.\n",
    "def addition(left, right):\n",
    "    new_data = left.data + right.data\n",
    "    return Tensor(new_data, requires_grad=True, grad_fn=AdditionNode(left, right))\n",
    "\n",
    "def substraction(left, right):\n",
    "    new_data = left.data - right.data\n",
    "    return Tensor(new_data, requires_grad=True, grad_fn=SubtractionNode(left, right))\n",
    "\n",
    "def matmul(left, right):\n",
    "    new_data = left.data.dot(right.data)\n",
    "    return Tensor(new_data, requires_grad=True, grad_fn=MatMulNode(left, right))\n",
    "\n",
    "def power(left, exponent):\n",
    "    new_data = left.data ** exponent\n",
    "    return Tensor(new_data, requires_grad=True, grad_fn=PowerNode(left, exponent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of addition (c): Tensor(array([4., 4., 4.]), requires_grad=True)\n",
      "Result of subtraction (d): Tensor(array([-2.,  0.,  2.]), requires_grad=True)\n",
      "Result of matrix multiplication (e): Tensor(array([0.]), requires_grad=True)\n",
      "Result of power operation (f): Tensor(array([0.]), requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = tensor(np.array([[2.0, 3.0]]))\n",
    "w1 = tensor(np.array([[1.0, 4.0]]), requires_grad=True)\n",
    "w2 = tensor(np.array([[3.0, -1.0]]), requires_grad=True)\n",
    "\n",
    "test_graph = x + w1 + w2\n",
    "\n",
    "print('Computational graph top node after x + w1 + w2:', test_graph.grad_fn)\n",
    "\n",
    "assert(isinstance(test_graph.grad_fn, AdditionNode))\n",
    "assert(test_graph.grad_fn.right is w2)\n",
    "assert(test_graph.grad_fn.left.grad_fn.left is x)\n",
    "assert(test_graph.grad_fn.left.grad_fn.right is w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Implementing the backward computations\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, *inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        raise NotImplementedError('Unimplemented')\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(id={id(self)})\"\n",
    "\n",
    "class AdditionNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        super().__init__(left, right)\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        if self.inputs[0].requires_grad:\n",
    "            self.inputs[0].backward(grad_output)\n",
    "        if self.inputs[1].requires_grad:\n",
    "            self.inputs[1].backward(grad_output)\n",
    "        \n",
    "\n",
    "class SubtractionNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        super().__init__(left, right)\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        if self.inputs[0].requires_grad:\n",
    "            self.inputs[0].backward(grad_output)\n",
    "        if self.inputs[1].requires_grad:\n",
    "            self.inputs[1].backward(grad_output)\n",
    "    \n",
    "class MatMulNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        super().__init__(left, right)\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if self.inputs[0].requires_grad:\n",
    "            grad_wrt_left = grad_output * self.inputs[1].data\n",
    "            self.inputs[0].backward(grad_wrt_left)\n",
    "        if self.inputs[1].requires_grad:\n",
    "            grad_wrt_right = grad_output * self.inputs[0].data\n",
    "            self.inputs[1].backward(grad_wrt_right)\n",
    "    \n",
    "class PowerNode(Node):\n",
    "    def __init__(self, base, exponent):\n",
    "        super().__init__(base)\n",
    "        self.base = base\n",
    "        self.exponent = exponent\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        base = self.inputs[0]\n",
    "        if base.requires_grad:\n",
    "            grad_wrt_base = grad_output * self.exponent * base.data ** (self.exponent - 1)\n",
    "            base.backward(grad_wrt_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of a: 16.0\n",
      "Gradient of b: 16.0\n"
     ]
    }
   ],
   "source": [
    "# Create input tensors\n",
    "a = tensor(np.array([2.0]), requires_grad=True)\n",
    "b = tensor(np.array([3.0]), requires_grad=True)\n",
    "\n",
    "# Perform operations\n",
    "c = a + b\n",
    "d = c - b\n",
    "e = c @ d\n",
    "f = e ** 2\n",
    "\n",
    "# Initiate backwards propogation\n",
    "f.backward()\n",
    "\n",
    "# Print the results\n",
    "print(\"Gradient of a:\", a.grad)\n",
    "print(\"Gradient of b:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Optimizers to update the model parameters\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDOptimizer:\n",
    "    def __init__(self, parameters, lr=0.01):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        # Update each parameter\n",
    "        for param in self.parameters:\n",
    "            if param.grad is not None:\n",
    "                param.data -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        # Reset gradients for each parameter\n",
    "        for param in self.parameters:\n",
    "            if param.grad is not None:\n",
    "                param.grad = np.zeros_like(param.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions and answers\n",
    "\n",
    "- Q1: Do we have to implement gradient accumulation?\n",
    "- Q2: Am I allowed to implement <some function>?\n",
    "- Q3: In Task 5, what do you mean by \"identical\"? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
