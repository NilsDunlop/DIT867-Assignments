{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e02c6b96b85df5a1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Assignment 4\n",
    "## Group Members:\n",
    "### Nils Dunlop, e-mail: gusdunlni@student.gu.se\n",
    "### Francisco Alejandro Erazo Piza, e-mail: guserafr@student.gu.se\n",
    "### Chukwudumebi Ubogu, e-mail: gusuboch@student.gu.se\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d86d47ab6dab10b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 1: A small linear regression example in PyTorch\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c27312703ea6f271",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T23:06:37.117887Z",
     "start_time": "2024-02-22T23:06:37.089889200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.76405235,  0.40015721],\n",
       "        [ 0.97873798,  2.2408932 ],\n",
       "        [ 1.86755799, -0.97727788],\n",
       "        [ 0.95008842, -0.15135721],\n",
       "        [-0.10321885,  0.4105985 ],\n",
       "        [ 0.14404357,  1.45427351],\n",
       "        [ 0.76103773,  0.12167502],\n",
       "        [ 0.44386323,  0.33367433],\n",
       "        [ 1.49407907, -0.20515826],\n",
       "        [ 0.3130677 , -0.85409574],\n",
       "        [-2.55298982,  0.6536186 ],\n",
       "        [ 0.8644362 , -0.74216502],\n",
       "        [ 2.26975462, -1.45436567],\n",
       "        [ 0.04575852, -0.18718385],\n",
       "        [ 1.53277921,  1.46935877],\n",
       "        [ 0.15494743,  0.37816252],\n",
       "        [-0.88778575, -1.98079647],\n",
       "        [-0.34791215,  0.15634897],\n",
       "        [ 1.23029068,  1.20237985],\n",
       "        [-0.38732682, -0.30230275],\n",
       "        [-1.04855297, -1.42001794],\n",
       "        [-1.70627019,  1.9507754 ],\n",
       "        [-0.50965218, -0.4380743 ],\n",
       "        [-1.25279536,  0.77749036],\n",
       "        [-1.61389785, -0.21274028],\n",
       "        [-0.89546656,  0.3869025 ],\n",
       "        [-0.51080514, -1.18063218],\n",
       "        [-0.02818223,  0.42833187],\n",
       "        [ 0.06651722,  0.3024719 ],\n",
       "        [-0.63432209, -0.36274117],\n",
       "        [-0.67246045, -0.35955316],\n",
       "        [-0.81314628, -1.7262826 ],\n",
       "        [ 0.17742614, -0.40178094],\n",
       "        [-1.63019835,  0.46278226],\n",
       "        [-0.90729836,  0.0519454 ],\n",
       "        [ 0.72909056,  0.12898291],\n",
       "        [ 1.13940068, -1.23482582],\n",
       "        [ 0.40234164, -0.68481009],\n",
       "        [-0.87079715, -0.57884966],\n",
       "        [-0.31155253,  0.05616534],\n",
       "        [-1.16514984,  0.90082649],\n",
       "        [ 0.46566244, -1.53624369],\n",
       "        [ 1.48825219,  1.89588918],\n",
       "        [ 1.17877957, -0.17992484],\n",
       "        [-1.07075262,  1.05445173],\n",
       "        [-0.40317695,  1.22244507],\n",
       "        [ 0.20827498,  0.97663904],\n",
       "        [ 0.3563664 ,  0.70657317],\n",
       "        [ 0.01050002,  1.78587049],\n",
       "        [ 0.12691209,  0.40198936],\n",
       "        [ 1.8831507 , -1.34775906],\n",
       "        [-1.270485  ,  0.96939671],\n",
       "        [-1.17312341,  1.94362119],\n",
       "        [-0.41361898, -0.74745481],\n",
       "        [ 1.92294203,  1.48051479],\n",
       "        [ 1.86755896,  0.90604466],\n",
       "        [-0.86122569,  1.91006495],\n",
       "        [-0.26800337,  0.8024564 ],\n",
       "        [ 0.94725197, -0.15501009],\n",
       "        [ 0.61407937,  0.92220667],\n",
       "        [ 0.37642553, -1.09940079],\n",
       "        [ 0.29823817,  1.3263859 ],\n",
       "        [-0.69456786, -0.14963454],\n",
       "        [-0.43515355,  1.84926373],\n",
       "        [ 0.67229476,  0.40746184],\n",
       "        [-0.76991607,  0.53924919],\n",
       "        [-0.67433266,  0.03183056],\n",
       "        [-0.63584608,  0.67643329],\n",
       "        [ 0.57659082, -0.20829876],\n",
       "        [ 0.39600671, -1.09306151],\n",
       "        [-1.49125759,  0.4393917 ],\n",
       "        [ 0.1666735 ,  0.63503144],\n",
       "        [ 2.38314477,  0.94447949],\n",
       "        [-0.91282223,  1.11701629],\n",
       "        [-1.31590741, -0.4615846 ],\n",
       "        [-0.06824161,  1.71334272],\n",
       "        [-0.74475482, -0.82643854],\n",
       "        [-0.09845252, -0.66347829],\n",
       "        [ 1.12663592, -1.07993151],\n",
       "        [-1.14746865, -0.43782004],\n",
       "        [-0.49803245,  1.92953205],\n",
       "        [ 0.94942081,  0.08755124],\n",
       "        [-1.22543552,  0.84436298],\n",
       "        [-1.00021535, -1.5447711 ],\n",
       "        [ 1.18802979,  0.31694261],\n",
       "        [ 0.92085882,  0.31872765],\n",
       "        [ 0.85683061, -0.65102559],\n",
       "        [-1.03424284,  0.68159452],\n",
       "        [-0.80340966, -0.68954978],\n",
       "        [-0.4555325 ,  0.01747916],\n",
       "        [-0.35399391, -1.37495129],\n",
       "        [-0.6436184 , -2.22340315],\n",
       "        [ 0.62523145, -1.60205766],\n",
       "        [-1.10438334,  0.05216508],\n",
       "        [-0.739563  ,  1.5430146 ],\n",
       "        [-1.29285691,  0.26705087],\n",
       "        [-0.03928282, -1.1680935 ],\n",
       "        [ 0.52327666, -0.17154633],\n",
       "        [ 0.77179055,  0.82350415],\n",
       "        [ 2.16323595,  1.33652795]]),\n",
       " array([ 0.53895342,  0.21594929,  0.85495138,  0.5636907 ,  0.32130011,\n",
       "         0.02282172,  0.43355955,  0.28494948,  0.62455331,  0.457357  ,\n",
       "        -0.09488842,  0.58031413,  0.94447131,  0.30172726,  0.32449838,\n",
       "         0.26373822,  0.38526199,  0.28157172,  0.62281853,  0.28318203,\n",
       "         0.20235347, -0.2192494 ,  0.2351877 ,  0.069649  , -0.0850409 ,\n",
       "         0.13153761,  0.37868332,  0.29131062,  0.23830441,  0.2282746 ,\n",
       "         0.10282648,  0.32232423,  0.3400722 ,  0.03356524,  0.04442704,\n",
       "         0.50787709,  0.79897741,  0.25672869,  0.27826382,  0.32854869,\n",
       "        -0.04091009,  0.52842224,  0.35116701,  0.51040517, -0.00891893,\n",
       "        -0.05060942,  0.36320961,  0.4108211 ,  0.04400282,  0.15071379,\n",
       "         0.8404956 , -0.06019488, -0.07903399,  0.29997196,  0.55319739,\n",
       "         0.60729503, -0.10907525,  0.04783283,  0.33979011,  0.3839887 ,\n",
       "         0.3860405 ,  0.17411641,  0.15875186,  0.03734816,  0.19584133,\n",
       "         0.14867472,  0.25425119,  0.14634071,  0.40984468,  0.51729792,\n",
       "         0.05805776,  0.00122176,  0.8168331 ,  0.08095776,  0.08182919,\n",
       "         0.08083589,  0.3336741 ,  0.36463529,  0.42725213,  0.37953982,\n",
       "         0.01203189,  0.57378817, -0.05040895,  0.47352592,  0.51573575,\n",
       "         0.50237034,  0.43274868,  0.19031785,  0.32825265,  0.37082139,\n",
       "         0.34825692,  0.40705651,  0.83290736,  0.02051509,  0.01094772,\n",
       "         0.18452787,  0.4515847 ,  0.4861225 ,  0.32067403,  0.57773763]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "data = pd.read_csv('a4_synthetic.csv')\n",
    "\n",
    "X = data.drop(columns='y').to_numpy()\n",
    "Y = data.y.to_numpy()\n",
    "\n",
    "(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2eccd0e22967383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T23:06:37.370905400Z",
     "start_time": "2024-02-22T23:06:37.125888100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE = 0.7999662647869263\n",
      "Epoch 2: MSE = 0.017392394159767264\n",
      "Epoch 3: MSE = 0.009377418162580966\n",
      "Epoch 4: MSE = 0.009355327616258364\n",
      "Epoch 5: MSE = 0.009365440349979508\n",
      "Epoch 6: MSE = 0.009366988411857164\n",
      "Epoch 7: MSE = 0.009367207068114567\n",
      "Epoch 8: MSE = 0.009367238481529512\n",
      "Epoch 9: MSE = 0.009367244712136654\n",
      "Epoch 10: MSE = 0.009367244620257224\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "w_init = np.random.normal(size=(2, 1))\n",
    "b_init = np.random.normal(size=(1, 1))\n",
    "\n",
    "# Declare the parameter tensors\n",
    "w = torch.tensor(w_init, dtype=torch.float32, requires_grad=True)\n",
    "b = torch.tensor(b_init, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "eta = 1e-2\n",
    "opt = optim.SGD([w, b], lr=eta)\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    sum_err = 0\n",
    "\n",
    "    for row in range(X.shape[0]):\n",
    "        x = torch.tensor(X[row, :], dtype=torch.float32).view(1, -1)\n",
    "        y = torch.tensor(Y[row], dtype=torch.float32).view(1, -1)\n",
    "\n",
    "        # Forward pass.\n",
    "        y_pred = x.mm(w) + b\n",
    "        err = (y_pred - y).pow(2).sum()\n",
    "\n",
    "        # Backward and update.\n",
    "        opt.zero_grad()\n",
    "        err.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # For statistics.\n",
    "        sum_err += err.item()\n",
    "\n",
    "    mse = sum_err / X.shape[0]\n",
    "    print(f'Epoch {i+1}: MSE =', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4539efc4250cf860",
   "metadata": {},
   "source": [
    "### Task 2: Implementing tensor arithmetics for forward computations\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e580b2c16c6713f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T23:06:37.397886300Z",
     "start_time": "2024-02-22T23:06:37.377883800Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "\n",
    "    # Constructor. Just store the input values.\n",
    "    def __init__(self, data, requires_grad=False, grad_fn=None):\n",
    "        self.data = data\n",
    "        self.shape = data.shape\n",
    "        self.grad_fn = grad_fn\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None\n",
    "\n",
    "    # So that we can print the object or show it in a notebook cell.\n",
    "    def __repr__(self):\n",
    "        dstr = repr(self.data)\n",
    "        if self.requires_grad:\n",
    "            gstr = ', requires_grad=True'\n",
    "        elif self.grad_fn is not None:\n",
    "            gstr = f', grad_fn={self.grad_fn}'\n",
    "        else:\n",
    "            gstr = ''\n",
    "        return f'Tensor({dstr}{gstr})'\n",
    "\n",
    "    # Extract one numerical value from this tensor.\n",
    "    def item(self):\n",
    "        return self.data.item()\n",
    "\n",
    "    # Operator +\n",
    "    def __add__(self, right):\n",
    "        if not isinstance(right, Tensor):\n",
    "            right = tensor(right)\n",
    "        return addition(self, right)\n",
    "\n",
    "    # Operator -\n",
    "    def __sub__(self, right):\n",
    "        if not isinstance(right, Tensor):\n",
    "            right = tensor(right)\n",
    "        return subtraction(self, right)\n",
    "\n",
    "    # Operator @\n",
    "    def __matmul__(self, right):\n",
    "        if not isinstance(right, Tensor):\n",
    "            raise ValueError(\"Right operand must be a Tensor\")\n",
    "        return matrix_multi(self, right)\n",
    "\n",
    "    # Operator **\n",
    "    def __pow__(self, right):\n",
    "        # NOTE! We are assuming that right is an integer here, not a Tensor!\n",
    "        if not isinstance(right, int):\n",
    "            raise Exception('only integers allowed')\n",
    "        if right < 2:\n",
    "            raise Exception('power must be >= 2')\n",
    "        return power(self, right)\n",
    "    \n",
    "    def sum(self):\n",
    "        sum_data = np.sum(self.data)\n",
    "        grad_fn = SummationNode(self) if self.requires_grad else None\n",
    "        return Tensor(sum_data, requires_grad=self.requires_grad, grad_fn=grad_fn)\n",
    "\n",
    "\n",
    "    # Backward computations. Will be implemented in Task 4.\n",
    "    def backward(self, grad_output=None):\n",
    "        if not self.requires_grad:\n",
    "            return\n",
    "\n",
    "        if grad_output is None:\n",
    "            grad_output = np.ones_like(self.data)\n",
    "\n",
    "        if self.grad is None:\n",
    "            self.grad = grad_output\n",
    "        else:\n",
    "            self.grad += grad_output\n",
    "\n",
    "        if self.grad_fn:\n",
    "            self.grad_fn.backward(grad_output)\n",
    "            \n",
    "# A small utility where we simply create a Tensor object\n",
    "def tensor(data, requires_grad=False):\n",
    "    return Tensor(data, requires_grad)\n",
    "\n",
    "# We define helper functions to implement the various arithmetic operations.\n",
    "\n",
    "# This function takes two tensors as input, and returns a new tensor holding\n",
    "# the result of an element-wise addition on the two input tensors.\n",
    "def addition(left, right):\n",
    "    new_data = left.data + right.data\n",
    "    grad_fn = grad_fn = AdditionNode(left, right) if left.requires_grad or right.requires_grad else None\n",
    "    return Tensor(new_data, requires_grad=left.requires_grad or right.requires_grad, grad_fn=grad_fn)\n",
    "\n",
    "def subtraction(left, right):\n",
    "    new_data = left.data - right.data\n",
    "    grad_fn = SubtractionNode(left, right) if left.requires_grad or right.requires_grad else None\n",
    "    return Tensor(new_data, requires_grad=left.requires_grad or right.requires_grad, grad_fn=grad_fn)\n",
    "\n",
    "def matrix_multi(left, right):\n",
    "    if left.shape[1] != right.shape[0]:\n",
    "        raise ValueError(\"Shapes are not compatible for matrix multiplication\")\n",
    "    new_data = np.dot(left.data, right.data)\n",
    "    grad_fn = MatMulNode(left, right) if left.requires_grad or right.requires_grad else None\n",
    "    return Tensor(new_data, requires_grad=left.requires_grad or right.requires_grad, grad_fn=grad_fn)\n",
    "\n",
    "def power(tensor, exponent):\n",
    "    new_data = tensor.data ** exponent\n",
    "    grad_fn = PowerNode(tensor, exponent) if tensor.requires_grad else None\n",
    "    return Tensor(new_data, requires_grad=tensor.requires_grad, grad_fn=grad_fn)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        raise NotImplementedError('Backward method not implemented.')\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(type(self))\n",
    "\n",
    "class AdditionNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        super().__init__()\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        if self.left.requires_grad:\n",
    "            if self.left.grad is None:\n",
    "                self.left.grad = grad_output\n",
    "            else:\n",
    "                self.left.grad += grad_output\n",
    "\n",
    "            if self.left.grad_fn:\n",
    "                self.left.grad_fn.backward(grad_output)\n",
    "        \n",
    "        if self.right.requires_grad:\n",
    "            if self.right.grad is None:\n",
    "                self.right.grad = grad_output\n",
    "            else:\n",
    "                self.right.grad += grad_output\n",
    "\n",
    "            if self.right.grad_fn:\n",
    "                self.right.grad_fn.backward(grad_output)\n",
    "\n",
    "\n",
    "class SubtractionNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        super().__init__()\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if self.left.requires_grad:\n",
    "            if self.left.grad is None:\n",
    "                self.left.grad = grad_output\n",
    "            else:\n",
    "                self.left.grad += grad_output\n",
    "\n",
    "            if self.left.grad_fn:\n",
    "                self.left.grad_fn.backward(grad_output)\n",
    "        \n",
    "        if self.right.requires_grad:\n",
    "            neg_grad_output = -grad_output\n",
    "            if self.right.grad is None:\n",
    "                self.right.grad = neg_grad_output\n",
    "            else:\n",
    "                self.right.grad += neg_grad_output\n",
    "\n",
    "            if self.right.grad_fn:\n",
    "                self.right.grad_fn.backward(neg_grad_output)\n",
    "\n",
    "class MatMulNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        super().__init__()\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if self.left.requires_grad:\n",
    "            grad_left = np.dot(grad_output, self.right.data.T)\n",
    "            if self.left.grad is None:\n",
    "                self.left.grad = grad_left\n",
    "            else:\n",
    "                self.left.grad += grad_left\n",
    "            if self.left.grad_fn:\n",
    "                self.left.grad_fn.backward(grad_left)\n",
    "\n",
    "        if self.right.requires_grad:\n",
    "            grad_right = np.dot(self.left.data.T, grad_output)\n",
    "            if self.right.grad is None:\n",
    "                self.right.grad = grad_right\n",
    "            else:\n",
    "                self.right.grad += grad_right\n",
    "            if self.right.grad_fn:\n",
    "                self.right.grad_fn.backward(grad_right)\n",
    "\n",
    "class PowerNode(Node):\n",
    "    def __init__(self, tensor, exponent):\n",
    "        super().__init__()\n",
    "        self.tensor = tensor\n",
    "        self.exponent = exponent\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if self.tensor.requires_grad:\n",
    "            # Compute the gradient of the power function w.r.t. its input\n",
    "            grad_tensor = self.exponent * (self.tensor.data ** (self.exponent - 1))\n",
    "\n",
    "            # Multiply by the incoming gradient\n",
    "            if self.tensor.grad is None:\n",
    "                self.tensor.grad = grad_output * grad_tensor\n",
    "            else:\n",
    "                self.tensor.grad += grad_output * grad_tensor\n",
    "\n",
    "            # Propagate the gradient to the tensor's grad_fn if it exists\n",
    "            if self.tensor.grad_fn:\n",
    "                self.tensor.grad_fn.backward(grad_output * grad_tensor)\n",
    "\n",
    "class SummationNode(Node):\n",
    "    def __init__(self, tensor):\n",
    "        super().__init__()\n",
    "        self.tensor = tensor\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if self.tensor.requires_grad:\n",
    "            if self.tensor.grad is None:\n",
    "                self.tensor.grad = np.ones_like(self.tensor.data) * grad_output\n",
    "            else:\n",
    "                self.tensor.grad += np.ones_like(self.tensor.data) * grad_output\n",
    "\n",
    "            if self.tensor.grad_fn:\n",
    "                self.tensor.grad_fn.backward(grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a0ef71c0d30a4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T23:06:37.429889100Z",
     "start_time": "2024-02-22T23:06:37.408889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test of addition: [[2. 3.]] + [[1. 4.]] = [[3. 7.]]\n",
      "Test of subtraction: [[2. 3.]] - [[1. 4.]] = [[ 1. -1.]]\n",
      "Test of power: [[1. 4.]] ** 2 = [[ 1. 16.]]\n",
      "Test of matrix multiplication: [[2. 3.]] @ [[-1. ]\n",
      " [ 1.2]] = [[1.6]]\n"
     ]
    }
   ],
   "source": [
    "# Two tensors holding row vectors.\n",
    "x1 = tensor(np.array([[2.0, 3.0]]))\n",
    "x2 = tensor(np.array([[1.0, 4.0]]))\n",
    "# A tensors holding a column vector.\n",
    "w = tensor(np.array([[-1.0], [1.2]]))\n",
    "\n",
    "# Test the arithmetic operations.\n",
    "test_plus = x1 + x2\n",
    "test_minus = x1 - x2\n",
    "test_power = x2 ** 2\n",
    "test_matmul = x1 @ w\n",
    "\n",
    "print(f'Test of addition: {x1.data} + {x2.data} = {test_plus.data}')\n",
    "print(f'Test of subtraction: {x1.data} - {x2.data} = {test_minus.data}')\n",
    "print(f'Test of power: {x2.data} ** 2 = {test_power.data}')\n",
    "print(f'Test of matrix multiplication: {x1.data} @ {w.data} = {test_matmul.data}')\n",
    "\n",
    "# Check that the results are as expected. Will crash if there is a miscalculation.\n",
    "assert(np.allclose(test_plus.data, np.array([[3.0, 7.0]])))\n",
    "assert(np.allclose(test_minus.data, np.array([[1.0, -1.0]])))\n",
    "assert(np.allclose(test_power.data, np.array([[1.0, 16.0]])))\n",
    "assert(np.allclose(test_matmul.data, np.array([[1.6]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e613def7599835",
   "metadata": {},
   "source": [
    "### Task 3: Building the computational graph\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "424df74ee3500e56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T23:06:37.508883100Z",
     "start_time": "2024-02-22T23:06:37.448887400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational graph top node after x + w1 + w2: <class '__main__.AdditionNode'>\n"
     ]
    }
   ],
   "source": [
    "x = tensor(np.array([[2.0, 3.0]]))\n",
    "w1 = tensor(np.array([[1.0, 4.0]]), requires_grad=True)\n",
    "w2 = tensor(np.array([[3.0, -1.0]]), requires_grad=True)\n",
    "\n",
    "test_graph = x + w1 + w2\n",
    "\n",
    "print('Computational graph top node after x + w1 + w2:', test_graph.grad_fn)\n",
    "\n",
    "assert(isinstance(test_graph.grad_fn, AdditionNode))\n",
    "assert(test_graph.grad_fn.right is w2)\n",
    "assert(test_graph.grad_fn.left.grad_fn.left is x)\n",
    "assert(test_graph.grad_fn.left.grad_fn.right is w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7db0907890df63",
   "metadata": {},
   "source": [
    "### Task 4: Implementing the backward computations\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d81b29112dc2895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T23:06:37.509885500Z",
     "start_time": "2024-02-22T23:06:37.464881300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of loss w.r.t. w =\n",
      " [[5.6]\n",
      " [8.4]]\n"
     ]
    }
   ],
   "source": [
    "x = tensor(np.array([[2.0, 3.0]]))\n",
    "w = tensor(np.array([[-1.0], [1.2]]), requires_grad=True)\n",
    "y = tensor(np.array([[0.2]]))\n",
    "\n",
    "# We could as well write simply loss = (x @ w - y)**2\n",
    "# We break it down into steps here if you need to debug.\n",
    "\n",
    "model_out = x @ w\n",
    "diff = model_out - y\n",
    "loss = diff ** 2\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('Gradient of loss w.r.t. w =\\n', w.grad)\n",
    "\n",
    "assert(np.allclose(w.grad, np.array([[5.6], [8.4]])))\n",
    "assert(x.grad is None)\n",
    "assert(y.grad is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "285d7cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.6000],\n",
       "        [8.4000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_x = torch.tensor(np.array([[2.0, 3.0]]))\n",
    "pt_w = torch.tensor(np.array([[-1.0], [1.2]]), requires_grad=True)\n",
    "pt_y = torch.tensor(np.array([[0.2]]))\n",
    "\n",
    "pt_model_out = pt_x @ pt_w\n",
    "pt_model_out.retain_grad() # Keep the gradient of intermediate nodes for debugging.\n",
    "\n",
    "pt_diff = pt_model_out - pt_y\n",
    "pt_diff.retain_grad()\n",
    "\n",
    "pt_loss = pt_diff ** 2\n",
    "pt_loss.retain_grad()\n",
    "\n",
    "pt_loss.backward()\n",
    "pt_w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb06c52184da60",
   "metadata": {},
   "source": [
    "### Task 5: Optimizers to update the model parameters\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3e92f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplementedError('Step method not implemented.')\n",
    "    \n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, lr):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for p in self.params:\n",
    "            p.data -= self.lr * p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec31cbb",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6044b240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE = 1.8310931353310658\n",
      "Epoch 2: MSE = 0.025294540833766224\n",
      "Epoch 3: MSE = 0.01007722900503592\n",
      "Epoch 4: MSE = 0.009120703993537975\n",
      "Epoch 5: MSE = 0.00904464904981755\n",
      "Epoch 6: MSE = 0.00903635318076267\n",
      "Epoch 7: MSE = 0.00903496660122323\n",
      "Epoch 8: MSE = 0.0090346589582873\n",
      "Epoch 9: MSE = 0.009034582893444464\n",
      "Epoch 10: MSE = 0.00903456348115694\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "w_init = np.random.normal(size=(2, 1))\n",
    "b_init = np.random.normal(size=(1, 1))\n",
    "\n",
    "w = tensor(w_init, requires_grad=True)\n",
    "b = tensor(b_init, requires_grad=True)\n",
    "\n",
    "optimizer = SGD([w, b], lr=1e-4)\n",
    "\n",
    "for i in range(10):\n",
    "    sum_err = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for row in range(X.shape[0]):\n",
    "        x = tensor(X[row, :].reshape(1, -1), requires_grad=False)\n",
    "        y = tensor(Y[row].reshape(1, -1), requires_grad=False)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = x @ w + b\n",
    "        err = ((y_pred - y) ** 2).sum()\n",
    "\n",
    "        # Backward pass\n",
    "        err.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # For statistics\n",
    "        sum_err += err.data\n",
    "\n",
    "    mse = sum_err / X.shape[0]\n",
    "    print(f'Epoch {i+1}: MSE = {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d62be",
   "metadata": {},
   "source": [
    "### Task 6: Classifying raisins\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e769920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# You may need to edit the path, depending on where you put the files.\n",
    "a4data = pd.read_csv('raisins.csv')\n",
    "\n",
    "X = scale(a4data.drop(columns='Class'))\n",
    "Y = 1.0*(a4data.Class == 'Besni').to_numpy()\n",
    "\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(len(Y))\n",
    "X = X[shuffle]\n",
    "Y = Y[shuffle]\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, random_state=0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6da213a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((720, 7), (720,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, Ytrain.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
