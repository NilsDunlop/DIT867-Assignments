{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e02c6b96b85df5a1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Assignment 4\n",
    "## Group Members:\n",
    "### Nils Dunlop, e-mail: gusdunlni@student.gu.se\n",
    "### Francisco Alejandro Erazo Piza, e-mail: guserafr@student.gu.se\n",
    "### Chukwudumebi Ubogu, e-mail: gusuboch@student.gu.se\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d86d47ab6dab10b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 1: A small linear regression example in PyTorch\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c27312703ea6f271",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T23:06:37.117887Z",
     "start_time": "2024-02-22T23:06:37.089889200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[ 1.76405235,  0.40015721],\n        [ 0.97873798,  2.2408932 ],\n        [ 1.86755799, -0.97727788],\n        [ 0.95008842, -0.15135721],\n        [-0.10321885,  0.4105985 ],\n        [ 0.14404357,  1.45427351],\n        [ 0.76103773,  0.12167502],\n        [ 0.44386323,  0.33367433],\n        [ 1.49407907, -0.20515826],\n        [ 0.3130677 , -0.85409574],\n        [-2.55298982,  0.6536186 ],\n        [ 0.8644362 , -0.74216502],\n        [ 2.26975462, -1.45436567],\n        [ 0.04575852, -0.18718385],\n        [ 1.53277921,  1.46935877],\n        [ 0.15494743,  0.37816252],\n        [-0.88778575, -1.98079647],\n        [-0.34791215,  0.15634897],\n        [ 1.23029068,  1.20237985],\n        [-0.38732682, -0.30230275],\n        [-1.04855297, -1.42001794],\n        [-1.70627019,  1.9507754 ],\n        [-0.50965218, -0.4380743 ],\n        [-1.25279536,  0.77749036],\n        [-1.61389785, -0.21274028],\n        [-0.89546656,  0.3869025 ],\n        [-0.51080514, -1.18063218],\n        [-0.02818223,  0.42833187],\n        [ 0.06651722,  0.3024719 ],\n        [-0.63432209, -0.36274117],\n        [-0.67246045, -0.35955316],\n        [-0.81314628, -1.7262826 ],\n        [ 0.17742614, -0.40178094],\n        [-1.63019835,  0.46278226],\n        [-0.90729836,  0.0519454 ],\n        [ 0.72909056,  0.12898291],\n        [ 1.13940068, -1.23482582],\n        [ 0.40234164, -0.68481009],\n        [-0.87079715, -0.57884966],\n        [-0.31155253,  0.05616534],\n        [-1.16514984,  0.90082649],\n        [ 0.46566244, -1.53624369],\n        [ 1.48825219,  1.89588918],\n        [ 1.17877957, -0.17992484],\n        [-1.07075262,  1.05445173],\n        [-0.40317695,  1.22244507],\n        [ 0.20827498,  0.97663904],\n        [ 0.3563664 ,  0.70657317],\n        [ 0.01050002,  1.78587049],\n        [ 0.12691209,  0.40198936],\n        [ 1.8831507 , -1.34775906],\n        [-1.270485  ,  0.96939671],\n        [-1.17312341,  1.94362119],\n        [-0.41361898, -0.74745481],\n        [ 1.92294203,  1.48051479],\n        [ 1.86755896,  0.90604466],\n        [-0.86122569,  1.91006495],\n        [-0.26800337,  0.8024564 ],\n        [ 0.94725197, -0.15501009],\n        [ 0.61407937,  0.92220667],\n        [ 0.37642553, -1.09940079],\n        [ 0.29823817,  1.3263859 ],\n        [-0.69456786, -0.14963454],\n        [-0.43515355,  1.84926373],\n        [ 0.67229476,  0.40746184],\n        [-0.76991607,  0.53924919],\n        [-0.67433266,  0.03183056],\n        [-0.63584608,  0.67643329],\n        [ 0.57659082, -0.20829876],\n        [ 0.39600671, -1.09306151],\n        [-1.49125759,  0.4393917 ],\n        [ 0.1666735 ,  0.63503144],\n        [ 2.38314477,  0.94447949],\n        [-0.91282223,  1.11701629],\n        [-1.31590741, -0.4615846 ],\n        [-0.06824161,  1.71334272],\n        [-0.74475482, -0.82643854],\n        [-0.09845252, -0.66347829],\n        [ 1.12663592, -1.07993151],\n        [-1.14746865, -0.43782004],\n        [-0.49803245,  1.92953205],\n        [ 0.94942081,  0.08755124],\n        [-1.22543552,  0.84436298],\n        [-1.00021535, -1.5447711 ],\n        [ 1.18802979,  0.31694261],\n        [ 0.92085882,  0.31872765],\n        [ 0.85683061, -0.65102559],\n        [-1.03424284,  0.68159452],\n        [-0.80340966, -0.68954978],\n        [-0.4555325 ,  0.01747916],\n        [-0.35399391, -1.37495129],\n        [-0.6436184 , -2.22340315],\n        [ 0.62523145, -1.60205766],\n        [-1.10438334,  0.05216508],\n        [-0.739563  ,  1.5430146 ],\n        [-1.29285691,  0.26705087],\n        [-0.03928282, -1.1680935 ],\n        [ 0.52327666, -0.17154633],\n        [ 0.77179055,  0.82350415],\n        [ 2.16323595,  1.33652795]]),\n array([ 0.53895342,  0.21594929,  0.85495138,  0.5636907 ,  0.32130011,\n         0.02282172,  0.43355955,  0.28494948,  0.62455331,  0.457357  ,\n        -0.09488842,  0.58031413,  0.94447131,  0.30172726,  0.32449838,\n         0.26373822,  0.38526199,  0.28157172,  0.62281853,  0.28318203,\n         0.20235347, -0.2192494 ,  0.2351877 ,  0.069649  , -0.0850409 ,\n         0.13153761,  0.37868332,  0.29131062,  0.23830441,  0.2282746 ,\n         0.10282648,  0.32232423,  0.3400722 ,  0.03356524,  0.04442704,\n         0.50787709,  0.79897741,  0.25672869,  0.27826382,  0.32854869,\n        -0.04091009,  0.52842224,  0.35116701,  0.51040517, -0.00891893,\n        -0.05060942,  0.36320961,  0.4108211 ,  0.04400282,  0.15071379,\n         0.8404956 , -0.06019488, -0.07903399,  0.29997196,  0.55319739,\n         0.60729503, -0.10907525,  0.04783283,  0.33979011,  0.3839887 ,\n         0.3860405 ,  0.17411641,  0.15875186,  0.03734816,  0.19584133,\n         0.14867472,  0.25425119,  0.14634071,  0.40984468,  0.51729792,\n         0.05805776,  0.00122176,  0.8168331 ,  0.08095776,  0.08182919,\n         0.08083589,  0.3336741 ,  0.36463529,  0.42725213,  0.37953982,\n         0.01203189,  0.57378817, -0.05040895,  0.47352592,  0.51573575,\n         0.50237034,  0.43274868,  0.19031785,  0.32825265,  0.37082139,\n         0.34825692,  0.40705651,  0.83290736,  0.02051509,  0.01094772,\n         0.18452787,  0.4515847 ,  0.4861225 ,  0.32067403,  0.57773763]))"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "data = pd.read_csv('a4_synthetic.csv')\n",
    "\n",
    "X = data.drop(columns='y').to_numpy()\n",
    "Y = data.y.to_numpy()\n",
    "\n",
    "(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T23:06:37.370905400Z",
     "start_time": "2024-02-22T23:06:37.125888100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE = 0.7999662647869263\n",
      "Epoch 2: MSE = 0.017392394159767264\n",
      "Epoch 3: MSE = 0.009377418162580966\n",
      "Epoch 4: MSE = 0.009355327616258364\n",
      "Epoch 5: MSE = 0.009365440349979508\n",
      "Epoch 6: MSE = 0.009366988411857164\n",
      "Epoch 7: MSE = 0.009367207068114567\n",
      "Epoch 8: MSE = 0.009367238481529512\n",
      "Epoch 9: MSE = 0.009367244712136654\n",
      "Epoch 10: MSE = 0.009367244620257224\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "w_init = np.random.normal(size=(2, 1))\n",
    "b_init = np.random.normal(size=(1, 1))\n",
    "\n",
    "# Declare the parameter tensors\n",
    "w = torch.tensor(w_init, dtype=torch.float32, requires_grad=True)\n",
    "b = torch.tensor(b_init, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "eta = 1e-2\n",
    "opt = optim.SGD([w, b], lr=eta)\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    sum_err = 0\n",
    "\n",
    "    for row in range(X.shape[0]):\n",
    "        x = torch.tensor(X[row, :], dtype=torch.float32).view(1, -1)\n",
    "        y = torch.tensor(Y[row], dtype=torch.float32).view(1, -1)\n",
    "\n",
    "        # Forward pass.\n",
    "        y_pred = x.mm(w) + b\n",
    "        err = (y_pred - y).pow(2).sum()\n",
    "\n",
    "        # Backward and update.\n",
    "        opt.zero_grad()\n",
    "        err.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # For statistics.\n",
    "        sum_err += err.item()\n",
    "\n",
    "    mse = sum_err / X.shape[0]\n",
    "    print(f'Epoch {i+1}: MSE =', mse)"
   ],
   "id": "a2eccd0e22967383"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Implementing tensor arithmetics for forward computations\n",
    "***"
   ],
   "id": "4539efc4250cf860"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T23:06:37.397886300Z",
     "start_time": "2024-02-22T23:06:37.377883800Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "\n",
    "    # Constructor. Just store the input values.\n",
    "    def __init__(self, data, requires_grad=False, grad_fn=None):\n",
    "        self.data = data\n",
    "        self.shape = data.shape\n",
    "        self.grad_fn = grad_fn\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None\n",
    "\n",
    "    # So that we can print the object or show it in a notebook cell.\n",
    "    def __repr__(self):\n",
    "        dstr = repr(self.data)\n",
    "        if self.requires_grad:\n",
    "            gstr = ', requires_grad=True'\n",
    "        elif self.grad_fn is not None:\n",
    "            gstr = f', grad_fn={self.grad_fn}'\n",
    "        else:\n",
    "            gstr = ''\n",
    "        return f'Tensor({dstr}{gstr})'\n",
    "\n",
    "    # Extract one numerical value from this tensor.\n",
    "    def item(self):\n",
    "        return self.data.item()\n",
    "\n",
    "    # Operator +\n",
    "    def __add__(self, right):\n",
    "        # Call the helper function defined below.\n",
    "        return addition(self, right)\n",
    "\n",
    "    # Operator -\n",
    "    def __sub__(self, right):\n",
    "        return subtraction(self, right)\n",
    "\n",
    "    # Operator @\n",
    "    def __matmul__(self, right):\n",
    "        return matmul(self, right)\n",
    "    \n",
    "    # Operator **\n",
    "    def __pow__(self, right):\n",
    "        # NOTE! We are assuming that right is an integer here, not a Tensor!\n",
    "        if not isinstance(right, int):\n",
    "            raise Exception('only integers allowed')\n",
    "        if right < 2:\n",
    "            raise Exception('power must be >= 2')\n",
    "        return power(self, right)\n",
    "\n",
    "    def backward(self, grad_output=None):\n",
    "        if self.requires_grad:\n",
    "            # Initialize grad_output if this is the end tensor\n",
    "            if grad_output is None:\n",
    "                grad_output = np.ones_like(self.data)\n",
    "\n",
    "            # Accumulate gradients\n",
    "            if self.grad is None:\n",
    "                self.grad = grad_output\n",
    "            else:\n",
    "                self.grad += grad_output\n",
    "\n",
    "            # Propagate gradients if this tensor is a result of an operation\n",
    "            if self.grad_fn is not None:\n",
    "                adjusted_grad_output = [self.grad_fn.compute_grad(grad_output, input_tensor) for input_tensor in self.grad_fn.inputs]\n",
    "\n",
    "                for input_tensor, grad in zip(self.grad_fn.inputs, adjusted_grad_output):\n",
    "                    input_tensor.backward(grad)"
   ],
   "id": "e580b2c16c6713f0"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T23:06:37.411884200Z",
     "start_time": "2024-02-22T23:06:37.387890700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the tensor object\n",
    "def tensor(data, requires_grad=False):\n",
    "    return Tensor(data, requires_grad)\n",
    "\n",
    "# Update the helper functions to implement the various arithmetic operations.\n",
    "def addition(left, right):\n",
    "    new_data = left.data + right.data\n",
    "    return Tensor(new_data, requires_grad=True, grad_fn=AdditionNode(left, right))\n",
    "\n",
    "def subtraction(left, right):\n",
    "    new_data = left.data - right.data\n",
    "    return Tensor(new_data, requires_grad=True, grad_fn=SubtractionNode(left, right))\n",
    "\n",
    "def matmul(left, right):\n",
    "    new_data = left.data.dot(right.data)\n",
    "    return Tensor(new_data, requires_grad=True, grad_fn=MatMulNode(left, right))\n",
    "\n",
    "def power(left, exponent):\n",
    "    new_data = left.data ** exponent\n",
    "    return Tensor(new_data, requires_grad=True, grad_fn=PowerNode(left, exponent))"
   ],
   "id": "6809482ec30139fd"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T23:06:37.429889100Z",
     "start_time": "2024-02-22T23:06:37.408889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test of addition: [[2. 3.]] + [[1. 4.]] = [[3. 7.]]\n",
      "Test of subtraction: [[2. 3.]] - [[1. 4.]] = [[ 1. -1.]]\n",
      "Test of power: [[1. 4.]] ** 2 = [[ 1. 16.]]\n",
      "Test of matrix multiplication: [[2. 3.]] @ [[-1. ]\n",
      " [ 1.2]] = [[1.6]]\n"
     ]
    }
   ],
   "source": [
    "# Two tensors holding row vectors.\n",
    "x1 = tensor(np.array([[2.0, 3.0]]))\n",
    "x2 = tensor(np.array([[1.0, 4.0]]))\n",
    "# A tensors holding a column vector.\n",
    "w = tensor(np.array([[-1.0], [1.2]]))\n",
    "\n",
    "# Test the arithmetic operations.\n",
    "test_plus = x1 + x2\n",
    "test_minus = x1 - x2\n",
    "test_power = x2 ** 2\n",
    "test_matmul = x1 @ w\n",
    "\n",
    "print(f'Test of addition: {x1.data} + {x2.data} = {test_plus.data}')\n",
    "print(f'Test of subtraction: {x1.data} - {x2.data} = {test_minus.data}')\n",
    "print(f'Test of power: {x2.data} ** 2 = {test_power.data}')\n",
    "print(f'Test of matrix multiplication: {x1.data} @ {w.data} = {test_matmul.data}')\n",
    "\n",
    "# Check that the results are as expected. Will crash if there is a miscalculation.\n",
    "assert(np.allclose(test_plus.data, np.array([[3.0, 7.0]])))\n",
    "assert(np.allclose(test_minus.data, np.array([[1.0, -1.0]])))\n",
    "assert(np.allclose(test_power.data, np.array([[1.0, 16.0]])))\n",
    "assert(np.allclose(test_matmul.data, np.array([[1.6]])))"
   ],
   "id": "b4a0ef71c0d30a4b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Building the computational graph\n",
    "***"
   ],
   "id": "55e613def7599835"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T23:06:37.444886600Z",
     "start_time": "2024-02-22T23:06:37.424882900Z"
    }
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, *inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        raise NotImplementedError('Unimplemented')\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(id={id(self)})\"\n",
    "\n",
    "class AdditionNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        super().__init__(left, right)\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def compute_grad(self, grad_output, input_tensor):\n",
    "        # Gradient does not change for addition\n",
    "        return grad_output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        adjusted_grad_output_left = self.compute_grad(grad_output, self.inputs[0])\n",
    "        adjusted_grad_output_right = self.compute_grad(grad_output, self.inputs[1])\n",
    "\n",
    "        if self.inputs[0].requires_grad:\n",
    "            self.inputs[0].backward(adjusted_grad_output_left)\n",
    "        if self.inputs[1].requires_grad:\n",
    "            self.inputs[1].backward(adjusted_grad_output_right)\n",
    "        \n",
    "\n",
    "class SubtractionNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        super().__init__(left, right)\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def compute_grad(self, grad_output, input_tensor):\n",
    "        if input_tensor == self.inputs[1]:\n",
    "            return -grad_output\n",
    "        return grad_output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.inputs[0].backward(self.compute_grad(grad_output, self.inputs[0]))\n",
    "        self.inputs[1].backward(self.compute_grad(grad_output, self.inputs[1]))\n",
    "        \n",
    "class MatMulNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        super().__init__(left, right)\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "\n",
    "    def compute_grad(self, grad_output, input_tensor):\n",
    "        if input_tensor == self.inputs[0]:\n",
    "            return grad_output.dot(self.inputs[1].data.T)\n",
    "        else:\n",
    "            return self.inputs[0].data.T.dot(grad_output)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.inputs[0].backward(self.compute_grad(grad_output, self.inputs[0]))\n",
    "        self.inputs[1].backward(self.compute_grad(grad_output, self.inputs[1]))\n",
    "    \n",
    "class PowerNode(Node):\n",
    "    def __init__(self, base, exponent):\n",
    "        super().__init__(base)\n",
    "        self.base = base\n",
    "        self.exponent = exponent\n",
    "\n",
    "    def compute_grad(self, grad_output, input_tensor):\n",
    "        return grad_output * self.exponent * (input_tensor.data ** (self.exponent - 1))\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        adjusted_grad_output = self.compute_grad(grad_output, self.inputs[0])\n",
    "        self.inputs[0].backward(adjusted_grad_output)"
   ],
   "id": "6ac0aea5238ecfdd"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T23:06:37.508883100Z",
     "start_time": "2024-02-22T23:06:37.448887400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational graph top node after x + w1 + w2: AdditionNode(id=2457500736992)\n"
     ]
    }
   ],
   "source": [
    "x = tensor(np.array([[2.0, 3.0]]))\n",
    "w1 = tensor(np.array([[1.0, 4.0]]), requires_grad=True)\n",
    "w2 = tensor(np.array([[3.0, -1.0]]), requires_grad=True)\n",
    "\n",
    "test_graph = x + w1 + w2\n",
    "\n",
    "print('Computational graph top node after x + w1 + w2:', test_graph.grad_fn)\n",
    "\n",
    "assert(isinstance(test_graph.grad_fn, AdditionNode))\n",
    "assert(test_graph.grad_fn.right is w2)\n",
    "assert(test_graph.grad_fn.left.grad_fn.left is x)\n",
    "assert(test_graph.grad_fn.left.grad_fn.right is w1)"
   ],
   "id": "424df74ee3500e56"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Implementing the backward computations\n",
    "***"
   ],
   "id": "ac7db0907890df63"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T23:06:37.509885500Z",
     "start_time": "2024-02-22T23:06:37.464881300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of loss w.r.t. w =\n",
      " [[5.6]\n",
      " [8.4]]\n"
     ]
    }
   ],
   "source": [
    "x = tensor(np.array([[2.0, 3.0]]))\n",
    "w = tensor(np.array([[-1.0], [1.2]]), requires_grad=True)\n",
    "y = tensor(np.array([[0.2]]))\n",
    "\n",
    "# We could as well write simply loss = (x @ w - y)**2\n",
    "# We break it down into steps here if you need to debug.\n",
    "\n",
    "model_out = x @ w\n",
    "diff = model_out - y\n",
    "loss = diff ** 2\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('Gradient of loss w.r.t. w =\\n', w.grad)\n",
    "\n",
    "assert(np.allclose(w.grad, np.array([[5.6], [8.4]])))\n",
    "assert(x.grad is None)\n",
    "assert(y.grad is None)"
   ],
   "id": "6d81b29112dc2895"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Optimizers to update the model parameters\n",
    "***"
   ],
   "id": "e3fb06c52184da60"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T23:06:37.546886400Z",
     "start_time": "2024-02-22T23:06:37.480884800Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGDOptimizer:\n",
    "    def __init__(self, parameters, lr=0.01):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        # Update each parameter\n",
    "        for param in self.parameters:\n",
    "            if param.grad is not None:\n",
    "                param.data -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        # Reset gradients for each parameter\n",
    "        for param in self.parameters:\n",
    "            if param.grad is not None:\n",
    "                param.grad = np.zeros_like(param.grad)"
   ],
   "id": "1017a431d31ffe68"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions and answers\n",
    "\n",
    "- Q1: Do we have to implement gradient accumulation?\n",
    "- Q2: Am I allowed to implement <some function>?\n",
    "- Q3: In Task 5, what do you mean by \"identical\"? "
   ],
   "id": "314e792731e8af8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
